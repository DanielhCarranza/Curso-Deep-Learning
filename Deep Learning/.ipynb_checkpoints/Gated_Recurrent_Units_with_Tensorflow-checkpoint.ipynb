{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Gated Recurrent Units with Tensorflow.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "id": "n5_JKBqMlnHa",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Recurrent Neural Networks \n",
        "\n",
        "Recurrent neural networks, or RNNs, are a family of neural networks for processing sequential data. Much as a convolutional networkis a neural network that is specialized for processing a grid of values Xsuch as an image, a recurrent neural network is a neural network that is specialized for processing a sequence of values $ x^{(1)} ,...x^{(T)}$ \n",
        "\n",
        "![alt text](https://cdn-images-1.medium.com/max/1600/1*4KwIUHWL3sTyguTahIxmJw.png)\n",
        "\n",
        "\n",
        "* $x_t$ is the input at time step t. For example, $x_1$ could be a one-hot vector corresponding to the second word of a sentence.\n",
        "* $h_t$ is the hidden state at time step t. It’s the “memory” of the network. $h_t$ is calculated based on the previous hidden state and the input at the current step: $h_t=f(Ux_t + Wh_{t-1})$. The function f usually is a nonlinearity such as tanh or ReLU.  $h_{0}$, which is required to calculate the first hidden state, is typically initialized to all zeros.\n",
        "* $y_t$ is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $y_t = \\mathrm{softmax}(Vs_t)$."
      ]
    },
    {
      "metadata": {
        "id": "F7ix4qm9lrlC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import time\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "% matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "JWR8yHwolrpe",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Enable Eager execution\n",
        "tf.enable_eager_execution()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "FucLHgBAlrto",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Data\n",
        "En este problema lo que queremos es generar nuevo texto basado en otro"
      ]
    },
    {
      "metadata": {
        "id": "WdmNJDKdlryI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Dataloader():\n",
        "  \"\"\" Load Text \"\"\"\n",
        "  def __init__(self):\n",
        "    \n",
        "    # Path to the file\n",
        "    path=tf.keras.utils.get_file('shakespeare.txt',\n",
        "            origin='https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "    \n",
        "    # Open file\n",
        "    with open(path, encoding='utf-8') as f:\n",
        "      self.raw_text=f.read()\n",
        "    \n",
        "    # Sorted list of the vocabulary  that contains all the unique characters in the file\n",
        "    self.chars=sorted(list(set(self.raw_text)))\n",
        "    \n",
        "    # Char to index\n",
        "    self.chars_idx={c:i for i,c in enumerate(self.chars)}\n",
        "    \n",
        "    # Idx to Char\n",
        "    self.idx_chars={i:c for i,c in enumerate(self.chars)}\n",
        "    \n",
        "    # Text \n",
        "    self.text=[self.chars_idx[c] for c in self.raw_text]\n",
        "    \n",
        "  def get_data(self,seq_length, batch_size, buffer_size):\n",
        "    input_text = []\n",
        "    target_text = []\n",
        "    \n",
        "    for f in range(0, len(self.text)-seq_length, seq_length):\n",
        "        \n",
        "        index=np.random.randint(0, len(self.text) - seq_length)\n",
        "        inps = self.raw_text[f:f+seq_length]\n",
        "        targ = self.raw_text[f+1:f+1+seq_length]\n",
        "\n",
        "        input_text.append([self.chars_idx[i] for i in inps])\n",
        "        target_text.append([self.chars_idx[t] for t in targ])\n",
        "        \n",
        "    inp,out=np.array(input_text),np.array(target_text)\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((inp, out)).shuffle(buffer_size)\n",
        "    dataset = dataset.batch(batch_size, drop_remainder=True)  \n",
        "    return dataset\n",
        "    \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "siMCiFIclr2r",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "data=Dataloader()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "miZdyLPylsFq",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Paramters"
      ]
    },
    {
      "metadata": {
        "id": "AHgU6o74vzFB",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# setting the maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "\n",
        "# length of the vocabulary in chars\n",
        "vocab_size = len(data.chars)\n",
        "\n",
        "# the embedding dimension \n",
        "embedding_dim = 256\n",
        "\n",
        "# number of RNN (here GRU) units\n",
        "units = 1024\n",
        "\n",
        "# batch size \n",
        "batch_size = 64\n",
        "\n",
        "# buffer size to shuffle our dataset\n",
        "BUFFER_SIZE = 10000"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "myDTSmxrCcn1",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Gated Recurrent Unit\n",
        "\n",
        "![alt text](https://stanford.edu/~shervine/images/gru.png)\n",
        "* $\\tilde{c}^{< t >}$\t= $\\textrm{tanh}(W_c[\\Gamma_r\\star a^{< t-1 >},x^{< t >}]+b_c)$\n",
        "\n",
        "* $c^{< t >}$\t= $\\Gamma_u\\star\\tilde{c}^{< t >}+(1-\\Gamma_u)\\star c^{< t-1 >}$\n",
        "* $a^{< t >}$ = \t$c^{< t >}$\n",
        "\n",
        "**Gates** :\n",
        " A system of gating units that controls the ﬂow of information\n",
        "* Update gate $\\Gamma_u$--> How much past should matter now?\n",
        "* Reveleance gate  $ \\Gamma_r$-->  Drop previous information?\n"
      ]
    },
    {
      "metadata": {
        "id": "SWYPcGjhFDtX",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class GRU(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
        "    super(GRU, self).__init__()\n",
        "    self.units = units\n",
        "    self.batch_sz = batch_size\n",
        "\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "\n",
        "    if tf.test.is_gpu_available():\n",
        "      self.gru = tf.keras.layers.CuDNNGRU(self.units, \n",
        "                                          return_sequences=True, \n",
        "                                          return_state=True, \n",
        "                                          recurrent_initializer='glorot_uniform')\n",
        "    else:\n",
        "      self.gru = tf.keras.layers.GRU(self.units, \n",
        "                                     return_sequences=True, \n",
        "                                     return_state=True, \n",
        "                                     recurrent_activation='sigmoid', \n",
        "                                     recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "  def call(self, x, hidden):\n",
        "    x = self.embedding(x)\n",
        "\n",
        "    # output shape == (batch_size, max_length, hidden_size) \n",
        "    # states shape == (batch_size, hidden_size)\n",
        "\n",
        "    output, states = self.gru(x, initial_state=hidden)\n",
        "\n",
        "    # reshaping   (batch_size * max_length, hidden_size)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "\n",
        "    # output shape after the dense layer == (max_length * batch_size, vocab_size)\n",
        "    x = self.fc(output)\n",
        "\n",
        "    return x, states\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "N-4vmOMfFOrP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "model=GRU(vocab_size, embedding_dim, units, batch_size=64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ddjiYM5Nysp6",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Optimizer and loss function"
      ]
    },
    {
      "metadata": {
        "id": "mwzKwBWIFOxJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "optimizer = tf.train.AdamOptimizer()\n",
        "\n",
        "# using sparse_softmax_cross_entropy so that we don't have to create one-hot vectors\n",
        "def loss_function(real, preds):\n",
        "    return tf.losses.sparse_softmax_cross_entropy(labels=real, logits=preds)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "edjIhTJjyowV",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Checkpoint"
      ]
    },
    {
      "metadata": {
        "id": "XgrO_fXtFO9c",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(optimizer=optimizer,\n",
        "                                 model=model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "NCysFLa1FPDN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Training the Model"
      ]
    },
    {
      "metadata": {
        "id": "4_EJDTw87xzo",
        "colab_type": "code",
        "outputId": "e78454e9-1855-417b-bbc2-97b6c5cf49bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1717
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "EPOCHS = 20\n",
        "dataset=data.get_data(100,64,1000)\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "    \n",
        "    # initializing the hidden state at the start of every epoch\n",
        "    hidden = model.reset_states()\n",
        "    \n",
        "    for (batch, (inp, target)) in enumerate(dataset):\n",
        "          with tf.GradientTape() as tape:\n",
        "              # feeding the hidden state back into the model\n",
        "              # This is the interesting step\n",
        "              predictions, hidden = model(inp, hidden)\n",
        "              \n",
        "              # reshaping the target because that's how the \n",
        "              # loss function expects it\n",
        "              target = tf.reshape(target, (-1,))\n",
        "              loss = loss_function(target, predictions)\n",
        "              \n",
        "          grads = tape.gradient(loss, model.variables)\n",
        "          optimizer.apply_gradients(zip(grads, model.variables))\n",
        "\n",
        "          if batch % 100 == 0:\n",
        "              print ('Epoch {} Batch {} Loss {:.4f}'.format(epoch+1,\n",
        "                                                            batch,\n",
        "                                                            loss))\n",
        "    # saving (checkpoint) the model every 5 epochs\n",
        "    if (epoch + 1) % 5 == 0:\n",
        "      checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "\n",
        "    print ('Epoch {} Loss {:.4f}'.format(epoch+1, loss))\n",
        "    print('Time taken for 1 epoch {} sec\\n'.format(time.time() - start))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 4.1745\n",
            "Epoch 1 Batch 100 Loss 2.3433\n",
            "Epoch 1 Loss 2.1014\n",
            "Time taken for 1 epoch 23.900368213653564 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 2.1628\n",
            "Epoch 2 Batch 100 Loss 1.9183\n",
            "Epoch 2 Loss 1.7826\n",
            "Time taken for 1 epoch 23.71653652191162 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 1.9324\n",
            "Epoch 3 Batch 100 Loss 1.6943\n",
            "Epoch 3 Loss 1.6095\n",
            "Time taken for 1 epoch 23.74864649772644 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 1.6453\n",
            "Epoch 4 Batch 100 Loss 1.5564\n",
            "Epoch 4 Loss 1.4745\n",
            "Time taken for 1 epoch 23.797654390335083 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 1.5787\n",
            "Epoch 5 Batch 100 Loss 1.4636\n",
            "Epoch 5 Loss 1.4647\n",
            "Time taken for 1 epoch 23.830735206604004 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 1.4679\n",
            "Epoch 6 Batch 100 Loss 1.4206\n",
            "Epoch 6 Loss 1.3856\n",
            "Time taken for 1 epoch 23.75021505355835 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 1.3861\n",
            "Epoch 7 Batch 100 Loss 1.4034\n",
            "Epoch 7 Loss 1.3404\n",
            "Time taken for 1 epoch 23.88285994529724 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 1.3551\n",
            "Epoch 8 Batch 100 Loss 1.3030\n",
            "Epoch 8 Loss 1.3339\n",
            "Time taken for 1 epoch 23.684443473815918 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 1.3480\n",
            "Epoch 9 Batch 100 Loss 1.3163\n",
            "Epoch 9 Loss 1.2606\n",
            "Time taken for 1 epoch 23.620137691497803 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 1.2965\n",
            "Epoch 10 Batch 100 Loss 1.2262\n",
            "Epoch 10 Loss 1.2622\n",
            "Time taken for 1 epoch 23.562912702560425 sec\n",
            "\n",
            "Epoch 11 Batch 0 Loss 1.2765\n",
            "Epoch 11 Batch 100 Loss 1.1894\n",
            "Epoch 11 Loss 1.2028\n",
            "Time taken for 1 epoch 23.619301795959473 sec\n",
            "\n",
            "Epoch 12 Batch 0 Loss 1.2001\n",
            "Epoch 12 Batch 100 Loss 1.1540\n",
            "Epoch 12 Loss 1.1941\n",
            "Time taken for 1 epoch 23.54275345802307 sec\n",
            "\n",
            "Epoch 13 Batch 0 Loss 1.2005\n",
            "Epoch 13 Batch 100 Loss 1.1578\n",
            "Epoch 13 Loss 1.1474\n",
            "Time taken for 1 epoch 23.57517695426941 sec\n",
            "\n",
            "Epoch 14 Batch 0 Loss 1.1584\n",
            "Epoch 14 Batch 100 Loss 1.1241\n",
            "Epoch 14 Loss 1.1083\n",
            "Time taken for 1 epoch 23.542822122573853 sec\n",
            "\n",
            "Epoch 15 Batch 0 Loss 1.1100\n",
            "Epoch 15 Batch 100 Loss 1.0738\n",
            "Epoch 15 Loss 1.0577\n",
            "Time taken for 1 epoch 23.712513208389282 sec\n",
            "\n",
            "Epoch 16 Batch 0 Loss 1.1008\n",
            "Epoch 16 Batch 100 Loss 1.0462\n",
            "Epoch 16 Loss 1.0605\n",
            "Time taken for 1 epoch 23.498114347457886 sec\n",
            "\n",
            "Epoch 17 Batch 0 Loss 1.1092\n",
            "Epoch 17 Batch 100 Loss 1.0252\n",
            "Epoch 17 Loss 1.0020\n",
            "Time taken for 1 epoch 23.45956540107727 sec\n",
            "\n",
            "Epoch 18 Batch 0 Loss 1.0410\n",
            "Epoch 18 Batch 100 Loss 0.9917\n",
            "Epoch 18 Loss 0.9906\n",
            "Time taken for 1 epoch 23.494333744049072 sec\n",
            "\n",
            "Epoch 19 Batch 0 Loss 1.0255\n",
            "Epoch 19 Batch 100 Loss 0.9716\n",
            "Epoch 19 Loss 0.9538\n",
            "Time taken for 1 epoch 23.54893660545349 sec\n",
            "\n",
            "Epoch 20 Batch 0 Loss 0.9781\n",
            "Epoch 20 Batch 100 Loss 0.9472\n",
            "Epoch 20 Loss 0.9186\n",
            "Time taken for 1 epoch 23.82195496559143 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "cgglQYjGd6nm",
        "colab_type": "code",
        "outputId": "1aa029fc-d4a8-49e8-97e3-2615a0dcd970",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.checkpointable.util.CheckpointLoadStatus at 0x7f8d8630dd30>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 52
        }
      ]
    },
    {
      "metadata": {
        "id": "qe2-8g7O6r5e",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Prediction\n"
      ]
    },
    {
      "metadata": {
        "id": "zTScoI0Z6sDq",
        "colab_type": "code",
        "outputId": "b23363fb-3ff6-4e48-dc79-362def12225b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 663
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "# number of characters to generate\n",
        "num_generate = 1000\n",
        "\n",
        "# You can change the start string to experiment\n",
        "start_string = 'Q'\n",
        "# converting our start string to numbers(vectorizing!) \n",
        "input_eval = [data.chars_idx[s] for s in start_string]\n",
        "input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "# empty string to store our results\n",
        "text_generated = ''\n",
        "\n",
        "# low temperatures results in more predictable text.\n",
        "# higher temperatures results in more surprising text\n",
        "# experiment to find the best setting\n",
        "temperature = 1.0\n",
        "\n",
        "# hidden state shape == (batch_size, number of rnn units); here batch size == 1\n",
        "hidden = [tf.zeros((1, units))]\n",
        "for i in range(num_generate):\n",
        "    predictions, hidden = model(input_eval, hidden)\n",
        "\n",
        "    # using a multinomial distribution to predict the word returned by the model\n",
        "    predictions = predictions / temperature\n",
        "    predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "    \n",
        "    # We pass the predicted word as the next input to the model\n",
        "    # along with the previous hidden state\n",
        "    input_eval = tf.expand_dims([predicted_id], 0)\n",
        "    \n",
        "    text_generated += data.idx_chars[predicted_id]\n",
        "\n",
        "print (start_string + text_generated)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "QUCESTER:\n",
            "The more my book shall steal upon the beast.\n",
            "\n",
            "LUCENTIO:\n",
            "Tranio, be so, because I will not do it.\n",
            "\n",
            "GRUMIO:\n",
            "I pray thee, mark me.\n",
            "\n",
            "PETRUCHIO:\n",
            "Now, by my charity, and well be satisfied\n",
            "With all the world can do no more than me?\n",
            "\n",
            "PROSPERO:\n",
            "Now I have spoke to her and supposed upon the body\n",
            "That come now to be shorten'd by our morning's son,\n",
            "A man of my state with her that should be thus bold\n",
            "to the prize and fellow in the sea with child\n",
            "Let me be made a fool to see the world she would come\n",
            "To see the wiser is ready.\n",
            "\n",
            "GRUMIO:\n",
            "The more my lord, be not angry.\n",
            "\n",
            "POMPEY:\n",
            "Pray, sir, he hath set the world shall proceed.\n",
            "\n",
            "PETRUCHIO:\n",
            "Now, by my charity, and well be satisfied\n",
            "With all the world can do no more than me?\n",
            "\n",
            "PROSPERO:\n",
            "Now I have spoke to her and supposed upon the body\n",
            "That come now to be shorten'd by our morning's son,\n",
            "A man of my state with her that should be thus bold\n",
            "to the prize and fellow in the sea with child\n",
            "Let me be made a fool to see the world she would come\n",
            "To see the w\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0wycTosH4VqQ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Jhx10YJN8-pb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}