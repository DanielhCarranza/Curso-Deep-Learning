{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gedLu2JxtRi4"
   },
   "source": [
    "# Red Neuronal Profunda\n",
    "\n",
    "\n",
    "**Notacion**:\n",
    "- Superscript $[l]$ denota la cantidad  asociada con la capa $l^{th}$ . \n",
    "    - Ejemplo: $a^{[L]}$ es la $L^{th}$ capa de activacion. $W^{[L]}$ y $b^{[L]}$ son los parametros de la capa $L^{th}$ .\n",
    "- Superscript $(i)$ denota la cantidad asociada con el  $i^{th}$ ejemplo. \n",
    "    - Ejemplo: $x^{(i)}$ es el $i^{th}$ ejemplor de entrenamiento.\n",
    "- Lowerscript $i$ denota la $i^{th}$ entrada del vector.\n",
    "    - Ejemplo: $a^{[l]}_i$ denota ls $i^{th}$ entrada a la capa de acticacion $l^{th}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObiQdt9rtRi6"
   },
   "source": [
    "## 1 - Importar modulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eySh3TyJtRi7"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "\n",
    "\n",
    "% matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wtNDi7N6eG-R"
   },
   "source": [
    "### Breast Cancer Dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "-CaJYMOGYzAi",
    "outputId": "95aed8c0-ecf6-4b6f-9e11-8faaafd1b72c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (455, 30)\n",
      "X_test shape: (114, 30)\n",
      "y_train shape: (455, 1)\n",
      "y_test shape: (114, 1)\n"
     ]
    }
   ],
   "source": [
    "cancer=load_breast_cancer()\n",
    "X=cancer.data\n",
    "Y=cancer.target\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train,X_test, y_train,y_test= train_test_split(X,Y,test_size=0.2, shuffle =True)\n",
    "\n",
    "print(f'X_train shape: {X_train.shape}')\n",
    "print(f'X_test shape: {X_test.shape}')\n",
    "\n",
    "y_train=y_train.reshape(-1,1)\n",
    "y_test=y_test.reshape(-1,1)\n",
    "\n",
    "print(f'y_train shape: {y_train.shape}')\n",
    "print(f'y_test shape: {y_test.shape}')\n",
    "\n",
    "y_train=y_train.reshape(-1,1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UeB3Ju5KeCQ7"
   },
   "source": [
    "### Mnist Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DodBF-OwkRDJ"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\compu\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.datasets import mnist\n",
    "(X_train,y_train),(X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train= X_train.reshape(-1, 28*28)\n",
    "y_train=y_train.reshape(-1,1)\n",
    "\n",
    "def one_hot_encoding(Y):\n",
    "  n_classes=len(np.unique(Y))\n",
    "  encoding= np.eye(n_classes)[Y.reshape(-1)]\n",
    "  return encoding\n",
    "\n",
    "Y= one_hot_encoding(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LT-B0DkRtRjE"
   },
   "source": [
    "## 2 - Inicializacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7wfMa31tRjP"
   },
   "source": [
    "### 2.1 - L-layer Neural Network\n",
    "\n",
    "construiremos una funcion para $L$ capas \n",
    " Recuerda que  $n^{[l]}$ es el  numero de unidades de la capa  $l$. Por ejepmplo si el tama√±o de la entrada es  $X$ con forma $(12288, 209)$ (donde $m=209$ ejemplos) entonces:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Shape of W** </td> \n",
    "        <td> **Shape of b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td> \n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td> **Layer 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td> **Layer 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    </tr>\n",
    "\n",
    "    <tr>\n",
    "        <td> **Layer L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    </tr>\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td> **Layer L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    </tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Recuerda que cuando computas  $W X + b$ en python, hace el broadcasting. Por ejemplo, si: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Entonces  $WX + b$ sera:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3xzDgcptRjR"
   },
   "source": [
    "\n",
    "**Instruciones**:\n",
    "- La estructura del modelo es:   *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e., que tiene  $L-1$ capas usando a ReLU activation seguida de un sigmoide\n",
    "- Usa random initialization para las matrices weight. \n",
    "- Usa zeros initialization para los  biases\n",
    "- Nosotros guardaremos el valor  $n^{[l]}$, que es el numero de las unidades en las diferentes capas , en  la variable `layer_dims`. Por example, una `layer_dims` de forma [2,4,1]: Eso significa que tenemos dos entradas, una hidden layer con 4 hidden units, y una output layer una  unidad ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w2mwABhstRjR"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- python array (list) que contiene las dimensiones de nuestra red neuronal\n",
    "    p.e [In, 100, 100,50,10]\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python diccionario que contiene los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix de la forma (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector de la forma (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    np.random.seed(3)\n",
    "    parametros={}\n",
    "    L =len(layer_dims) # numero de capas\n",
    "    \n",
    "    for l in range(1, L):\n",
    "        parametros['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01 # Forma W: (N[l], N[l-1])\n",
    "        parametros['b' + str(l)] = np.zeros((layer_dims[l],1)) # Forma Bias : (N[l], 1)\n",
    "        \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "ev5-jFlytRjU",
    "outputId": "7bbddab5-d868-4d1f-8cd1-77e5dde46ded"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'W1': array([[ 0.01788628,  0.0043651 ,  0.00096497, -0.01863493, -0.00277388],\n",
       "        [-0.00354759, -0.00082741, -0.00627001, -0.00043818, -0.00477218],\n",
       "        [-0.01313865,  0.00884622,  0.00881318,  0.01709573,  0.00050034],\n",
       "        [-0.00404677, -0.0054536 , -0.01546477,  0.00982367, -0.01101068]]),\n",
       " 'W2': array([[-0.01185047, -0.0020565 ,  0.01486148,  0.00236716],\n",
       "        [-0.01023785, -0.00712993,  0.00625245, -0.00160513],\n",
       "        [-0.00768836, -0.00230031,  0.00745056,  0.01976111]]),\n",
       " 'b1': array([[0.],\n",
       "        [0.],\n",
       "        [0.],\n",
       "        [0.]]),\n",
       " 'b2': array([[0.],\n",
       "        [0.],\n",
       "        [0.]])}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param= initialize_parameters_deep([5,4,3])\n",
    "param"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDAFYRL-tRjX"
   },
   "source": [
    "## 3 - Forward propagation modulo\n",
    "\n",
    "### 3.1 - Linear Forward \n",
    "Modulo Linear Foward:\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION donde  ACTIVATION puede ser ReLU o Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID \n",
    "\n",
    "La funcion vectorizada para computar el Foward prop modulo sobre todos los ejemplos es:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "donde $A^{[0]} = X$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khkbmfnGtRjY"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implementa el modulo de forward propagation para Lth capas.\n",
    "\n",
    "    Argumentos:\n",
    "    A -- activaciones de la capa anterior(o datos de entrada): (Tama√±o de la capa anterior, numero de ejemplos)\n",
    "    W -- weights matrix: numpy array de la forma (Tama√±o de la capa actual, Tama√±o de la capa anterior)\n",
    "    b -- bias vector, numpy array de la forma (Tama√±o de la capa actual, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- la entrada de la activacion \n",
    "    cache --  \"A\", \"W\" y \"b\" \n",
    "    \"\"\"\n",
    "    # Transformacion lineal\n",
    "    Z =W @ A + b\n",
    "    \n",
    "    cache=(A,W,b)\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-TSSecrQHry4"
   },
   "outputs": [],
   "source": [
    "param= initialize_parameters_deep([784,4,3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PeECgzDtRjf"
   },
   "source": [
    "### 3.2 - Linear-Activation Forward\n",
    "\n",
    "Funciones de activacion\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: La formula matematica  para ReLu es $A = RELU(Z) = max(0, Z)$. \n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tpqh7WghtRjg"
   },
   "source": [
    "Implementar el paso de  forward propagation para la capa  *LINEAR->ACTIVATION* . \n",
    "\n",
    "La relacion matematica es la siguiente: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ donde   \"g\"  es la fn activacion que puede ser ya sea sigmoid() o relu(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oiH6KWQ6J025"
   },
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "  return 1.0/(1 + np.exp(-np.clip(Z, -250, 250)))\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"Funcion de activacion ReLu\"\"\"\n",
    "    A=np.maximum(0,Z)\n",
    "    return A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMoZo7VttRjg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implementar el paso de  forward propagation para la capa LINEAR->ACTIVATION \n",
    "\n",
    "    Argumentos:\n",
    "    A_prev -- activaciones de la capa anterior(o datos de entrada): (Tama√±o de la capa anterior, numero de ejemplos)\n",
    "    W -- weights matrix: numpy array de la forma (Tama√±o de la capa actual, Tama√±o de la capa anterior)\n",
    "    b -- bias vector, numpy array de la forma (Tama√±o de la capa actual, 1)\n",
    "    activation -- la fn de  activacion que sera usada en esta capa\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    A -- post-activacion \n",
    "    cache --  diccionario \"linear_cache\" y \"activation_cache\";\n",
    "             \n",
    "    \"\"\"\n",
    "    Z, cache= linear_forward(A_prev, W, b)\n",
    "    \n",
    "    if activation == \"sigmoid\":\n",
    "        A = sigmoid(Z)\n",
    "        \n",
    "    else:\n",
    "        A = relu(Z) # Rectifier Linear Unit\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b7Viz8-5JnTb"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kT1OgS00tRjo"
   },
   "source": [
    "### 3.3 Modelo de L capas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88eb9kQEtRjw"
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parametros):\n",
    "    \"\"\"\n",
    "    Implementa forward propagation para [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID \n",
    "    \n",
    "    Arguments:\n",
    "    X -- datos, numpy array de shape (Tama√±o de entrada, numero de ejemplos)\n",
    "    parametros -- salida de  initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL --  valor de post-activacion \n",
    "    caches \n",
    "    \"\"\"\n",
    "    caches=[]\n",
    "    A=X# X=  A^[0] primera capa\n",
    "    L= len(parametros)//2  # numero de capas o layers\n",
    "    \n",
    "    # Linear --> Relu\n",
    "    for l in range(1,L):\n",
    "        # X\n",
    "        #activation= 'relu ' if l< L else 'sigmoid'\n",
    "        A, cache =linear_activation_forward(A, parametros['W' + str(l)], parametros['b' + str(l)],activation='relu')\n",
    "        # guarda Z ,A , etc que nos sirve para el backward prop\n",
    "        caches.append(cache)\n",
    "\n",
    "    AL, cache = linear_activation_forward(A, parametros[\"W\" + str(L)], parametros[\"b\"+str(L)], activation= 'sigmoid')\n",
    "    caches.append(cache)\n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mISbTkYMtRj2"
   },
   "source": [
    "## 4 - Cost function\n",
    "\n",
    "\n",
    "**Crea la funcion**: Categorical cross-entropy cost \n",
    "$$J=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))¬†\\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DuTcXaqtRj4"
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implementa la cost function  (7).\n",
    "\n",
    "    Argumentoss:\n",
    "    AL -- vector de probabilidad correspondinte a las predicciones, shape (1, numero de ejemplos)\n",
    "    Y -- vector de  clases o categorias a predecir, shape (1, numero de ejemplos)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    eps = np.finfo(float).eps\n",
    "    m= Y.shape[0]\n",
    "    loss= -np.sum(Y @ np.log(AL.T + eps) + (1-Y) @ np.log(1-AL.T + eps))/m\n",
    "    \n",
    "    return np.squeeze(loss) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLuT8r6LtRj9"
   },
   "source": [
    "## 5 - Backward propagation modulo\n",
    "\n",
    "\n",
    "\n",
    "**Recordatorio sobre backprop** \n",
    "Recuerda que para encontrar los gradientes tienes que computar la derivada parcial $\\mathcal{L}$ con respecto a los parametros por ejemplo $z^{[1]}$ para una rnn de dos capas:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "\n",
    "\n",
    "Ahora computaremos el modulo de backprop en tres pasos:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward donde ACTIVATION computa la derivada de  ReLU o sigmoid \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCbFW-EMtRj-"
   },
   "source": [
    "### 5.1 - Linear backward\n",
    "\n",
    "Formulas vectorizada para el modulo de bp\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6kaKIaEtRkE"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implementa el modulo para  backward propagation para una capa\n",
    "\n",
    "    Argumentos:\n",
    "    dZ -- Gradiente del cost con respecto a la capa actual \n",
    "    cache -- tupla de los valores (A_prev, W, b) proveninetes del  forward propagation en la capa actual\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradiente del cost con respecto a la activacion\n",
    "    dW -- Gradiente del  cost con respecto a W (capa actual l)\n",
    "    db -- Gradiente del  cost con respecto a b (capa actual l)\n",
    "    \"\"\"\n",
    "    A_prev,W,b = cache\n",
    "    m =A_prev.shape[0]\n",
    "    \n",
    "    dW =(dZ @ A_prev.T)/m # misma forma que W\n",
    "    db= np.sum(dZ,axis=1, keepdims=True)/m # misma forma que b\n",
    "    dA_prev =W.T @ dZ # misma forma que A_prev\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RumwjG6ktRkI"
   },
   "source": [
    "### 5.2 - Linear-Activation backward\n",
    "\n",
    "\n",
    "\n",
    "Si $g(.)$ es la  fn activacion \n",
    "`sigmoid_backward` y `relu_backward` computan  la derivada de la fn actvivacion $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "Implementa la capa de  backpropagation  *LINEAR->ACTIVATION* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgUSHulqtRkJ"
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implementa la capa de backward propagation para LINEAR->ACTIVATION\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- gradiente de post-activacion de l capa actual l \n",
    "    cache -- tuple de valores (linear_cache, activation_cache) \n",
    "    activation -- fn activacion\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient del cost con respecto a la fn de activacion (de la capa anterior l-1)\n",
    "    dW -- Gradiente del  cost con respecto a W (capa actual l)\n",
    "    db -- Gradiente del  cost con respecto a b (capa actual l)\n",
    "    \"\"\"\n",
    "    # Derivada de la funcion de acticacion Relu\n",
    "    if activation =='relu':\n",
    "        # si X>0 ponlo 1 sino 0\n",
    "        dZ = (dA>0)\n",
    "        \n",
    "    # Derivada de la funcion de acticacion sigmoide\n",
    "    elif activation =='sigmoid':\n",
    "        # s=s*(1-s)\n",
    "        s= sigmoid(dA) # sigmoid \n",
    "        dZ= dA*s*(1-s)\n",
    "    \n",
    "    dA_prev,dW,db=linear_backward(dZ,cache)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSn-f6o9tRkP"
   },
   "source": [
    "### 5.3 - L-Modelo Backward \n",
    "\n",
    "\n",
    "** Inicializacion de backpropagation**:\n",
    "Para hacer backprop atraves de la rednn, sabemos que la salida es, \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Y la derivada de su funcion de error con tespecto a A  es : $dA^{[L]}= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "\n",
    "donde $dA^{[L]}= \\frac YA + \\frac{(1-Y)}{(1-A)}$\n",
    "\n",
    "una forma de hacerlo con numpy es :\n",
    "\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3bieU7pHtRkQ"
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implementa el modulo de backward propagation para  [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    \n",
    "    Argumentos:\n",
    "    AL -- vector de probabilidad, salida del forward propagation (L_model_forward())\n",
    "    Y -- clases \n",
    "    caches -- \n",
    "    \n",
    "    Returns:\n",
    "    grads -- Un diccionario de los gradientes\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads={}\n",
    "    L=len(caches) # numero de capas\n",
    "    m =AL.shape[0]\n",
    "    eps = np.finfo(float).eps\n",
    "    \n",
    "    # Derivada de Loss con respecto a A \n",
    "    dAL =  - (np.divide(Y, AL + eps ) - np.divide(1 - Y, 1 - AL+ eps))\n",
    "    \n",
    "    # Inicializar Gradientes\n",
    "    # Backprop Sigmoide --> linear\n",
    "    grads['dA'+ str(L)], grads['dW'+str(L)], grads['db'+ str(L)] = linear_activation_backward(dAL,caches[L-1],activation='sigmoid')\n",
    "    \n",
    "    # Backprop Relu ---> Linear\n",
    "    for l in reversed(range(L-1)):\n",
    "       # current_cache = caches[l]\n",
    "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA' + str(l+2)], caches[l], activation = \"relu\")\n",
    "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
    "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
    "        grads[\"db\" + str(l + 1)] = db_temp\n",
    "    \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFwZVaFAtRkZ"
   },
   "source": [
    "## 6  - Algoritmo de optimizacion \n",
    "\n",
    "Gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "donde  $\\alpha$ es el  learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOWS38xjtRka"
   },
   "outputs": [],
   "source": [
    "def update_parameters(parametros, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Gradient descent\n",
    "    \n",
    "    Argumentos:\n",
    "    parametros  \n",
    "    grads  \n",
    "    \n",
    "    Returns:\n",
    "    parametros -- actualizados \n",
    "    \"\"\"\n",
    "    # Numero de capas\n",
    "    L = len(parametros)//2 \n",
    "\n",
    "    for l in range(1,L):\n",
    "      parametros['W' + str(l)] += - learning_rate*grads['dW' + str(l)]\n",
    "      parametros['b' + str(l)] += - learning_rate*grads['db' + str(l)]\n",
    "    \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YhTUYO1TMLUm"
   },
   "source": [
    "## Aplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "5AffIjcxCPy3"
   },
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): \n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []       \n",
    "    \n",
    "    # Parameters initialization.\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "        \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y)\n",
    "        \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Update parametros\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                \n",
    "        # Print the cost every 100 training example\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "            \n",
    "    # plot the cost\n",
    "    plt.plot(np.squeeze(costs))\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('iterations (per tens)')\n",
    "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
    "    plt.show()\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LHn5X6nICQkM"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 415883.136818\n",
      "Cost after iteration 100: 415888.002026\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-21-c139c787717f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mlayers_dims\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m784\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m20\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m7\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m10\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;31m#  5-layer model\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mparametros\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mL_layer_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mT\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlayers_dims\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mlearning_rate\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.01\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_iterations\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2500\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprint_cost\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-20-8e5f514f75d7>\u001b[0m in \u001b[0;36mL_layer_model\u001b[1;34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost)\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m         \u001b[1;31m# Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m         \u001b[0mAL\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaches\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mL_model_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     16\u001b[0m         \u001b[1;31m# Compute cost.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-10-0802f9a56c1e>\u001b[0m in \u001b[0;36mL_model_forward\u001b[1;34m(X, parametros)\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[1;31m# X\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m         \u001b[1;31m#activation= 'relu ' if l< L else 'sigmoid'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mlinear_activation_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparametros\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'W'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparametros\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'b'\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mactivation\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'relu'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m         \u001b[1;31m# guarda Z ,A , etc que nos sirve para el backward prop\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m         \u001b[0mcaches\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcache\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-9-8a6a785e9bd8>\u001b[0m in \u001b[0;36mlinear_activation_forward\u001b[1;34m(A_prev, W, b, activation)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \"\"\"\n\u001b[1;32m---> 18\u001b[1;33m     \u001b[0mZ\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mlinear_forward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA_prev\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     19\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"sigmoid\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-6-09c357328c17>\u001b[0m in \u001b[0;36mlinear_forward\u001b[1;34m(A, W, b)\u001b[0m\n\u001b[0;32m     13\u001b[0m     \"\"\"\n\u001b[0;32m     14\u001b[0m     \u001b[1;31m# Transformacion lineal\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m     \u001b[0mZ\u001b[0m \u001b[1;33m=\u001b[0m\u001b[0mW\u001b[0m \u001b[1;33m@\u001b[0m \u001b[0mA\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mb\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[0mcache\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mA\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mW\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "layers_dims = [784, 20, 7, 5, 10] #  5-layer model\n",
    "parametros= L_layer_model(X_train.T, Y.T, layers_dims,learning_rate=0.01, num_iterations=2500, print_cost=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_uVkHFCMdIN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EGGHKFbtCQsO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyZx1h4otRkg"
   },
   "source": [
    "## 7 Prediccion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4X3feqp8yP9"
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parametros):\n",
    "    \"\"\"\n",
    "    .\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- data set \n",
    "    parametros\n",
    "    \n",
    "    Returns:\n",
    "    p -- predicciones del dataset dado X\n",
    "    \"\"\"\n",
    "    \n",
    "    \n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mefIcTtE8xTw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rP504iM8xa_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Wj6g0IMa9MtC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uvCqKA3N9Mza"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Red_Neuronal_Profunda.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
