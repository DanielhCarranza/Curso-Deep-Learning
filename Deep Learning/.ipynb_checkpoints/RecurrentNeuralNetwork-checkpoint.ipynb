{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks, or RNNs, are a family of neural networks for processing sequential data. Much as a convolutional networkis a neural network that is specialized for processing a grid of values Xsuch as an image, a recurrent neural network is a neural network that is specialized for processing a sequence of values $ x^{(1)} ,...x^{(T)}$ \n",
    "\n",
    "![alt text](https://cdn-images-1.medium.com/max/1600/1*4KwIUHWL3sTyguTahIxmJw.png)\n",
    "\n",
    "\n",
    "* $x_t$ is the input at time step t. For example, $x_1$ could be a one-hot vector corresponding to the second word of a sentence.\n",
    "* $h_t$ is the hidden state at time step t. It’s the “memory” of the network. $h_t$ is calculated based on the previous hidden state and the input at the current step: $h_t=f(Ux_t + Wh_{t-1})$. The function f usually is a nonlinearity such as tanh or ReLU.  $h_{0}$, which is required to calculate the first hidden state, is typically initialized to all zeros.\n",
    "* $y_t$ is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $y_t = \\mathrm{softmax}(Vs_t)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import os\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Dataloader():\n",
    "  \"\"\" Load Text \"\"\"\n",
    "  def __init__(self):\n",
    "    \n",
    "    # Path to the file\n",
    "    path=tf.keras.utils.get_file('nietzsche.txt',\n",
    "            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')\n",
    "    \n",
    "    # Open file\n",
    "    with open(path, encoding='utf-8') as f:\n",
    "      self.raw_text=f.read().lower()\n",
    "    \n",
    "    # Sorted list of the vocabulary  that contains all the unique characters in the file\n",
    "    self.chars=sorted(list(set(self.raw_text)))\n",
    "    \n",
    "    # Char to index\n",
    "    self.chars_idx={c:i for i,c in enumerate(self.chars)}\n",
    "    \n",
    "    # Idx to Char\n",
    "    self.idx_chars={i:c for i,c in enumerate(self.chars)}\n",
    "    \n",
    "    # Text \n",
    "    self.text=[self.chars_idx[c] for c in self.raw_text]\n",
    "    \n",
    "  def get_batch(self,seq_length, batch_size):\n",
    "    seq=[]\n",
    "    next_char=[]\n",
    "    \n",
    "    for i in range(batch_size):\n",
    "      index=np.random.randint(0, len(self.text) - seq_length)\n",
    "      seq.append(self.text[index:index + seq_length])\n",
    "      next_char.append(self.text[index + seq_length])\n",
    "    \n",
    "    X,state= np.array(seq),  np.array(next_char)\n",
    "    #setattr(X.dtype,'base_dtype',X.dtype)\n",
    "    #X.dtype.__setattr__('base_dtype', X.dtype)\n",
    "    return X, state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=Dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the dataset \n",
    "path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(path_to_file, encoding='utf-8') as f:\n",
    "      raw_text=f.read().lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parametros "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maximum length\n",
    "seq_length = 100\n",
    "vocab_size= len(data.text)\n",
    "embedding_dim= 256\n",
    "units= 1024\n",
    "batch_size=64\n",
    "buffer_size=10000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(tf.keras.Model):\n",
    "    \"\"\"Long Short Term Memory\"\"\"\n",
    "    \n",
    "    def __init__(self,vocab_size, embedding_dim, units, batch_size):\n",
    "        super(LSTM,self).__init__()\n",
    "        \n",
    "        # Parametros\n",
    "        self.units=units\n",
    "        self.batch_sz=batch_size\n",
    "        \n",
    "        # Word Embeddings\n",
    "        self.embedding= tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Select device\n",
    "        params={'return_sequences':True, 'return_state':True, 'recurrent_initializer':'glorot_uniform'}\n",
    "        if tf.test.is_gpu_available():\n",
    "            self.lstm= tf.keras.layers.CuDNNLSTM(self.units,**params)\n",
    "        else: \n",
    "            self.lstm= tf.keras.layers.LSTM(self.units,**params)\n",
    "            \n",
    "        self.fc= tf.keras.layers.Dense(batch_size) # Y= WX + b\n",
    "        \n",
    "    def call(self, x, hidden_state):\n",
    "        # Word embedding\n",
    "        x= self.embedding(tf.convert_to_tensor(x))\n",
    "        \n",
    "        # output shape == (batch_size, seq_length, hidden_state)\n",
    "        # state shpe == (batch_size, hidden_state)\n",
    "        output, states, _= self.lstm(x, initial_state=hidden_state)\n",
    "        \n",
    "        # Reshape del output : (batch_size*seq_length, hidden_state)\n",
    "        output= tf.reshape(output, (-1, output.shape[2]))\n",
    "        x= self.fc(output)\n",
    "        return x, states\n",
    "    \n",
    "        \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo= LSTM(vocab_size,embedding_dim, units,batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer= tf.train.AdamOptimizer()\n",
    "\n",
    "# loss function\n",
    "def loss_function(real,preds):\n",
    "    return tf.losses.sparse_softmax_cross_entropy(labels=real,logits=preds)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'transpose_5:0' shape=(64, 6400) dtype=float32>"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.transpose(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch : 0, loss: Tensor(\"sparse_softmax_cross_entropy_loss_38/value:0\", shape=(), dtype=float32)\n",
      "epoch : 1, loss: Tensor(\"sparse_softmax_cross_entropy_loss_39/value:0\", shape=(), dtype=float32)\n",
      "epoch : 2, loss: Tensor(\"sparse_softmax_cross_entropy_loss_40/value:0\", shape=(), dtype=float32)\n",
      "epoch : 3, loss: Tensor(\"sparse_softmax_cross_entropy_loss_41/value:0\", shape=(), dtype=float32)\n",
      "epoch : 4, loss: Tensor(\"sparse_softmax_cross_entropy_loss_42/value:0\", shape=(), dtype=float32)\n",
      "epoch : 5, loss: Tensor(\"sparse_softmax_cross_entropy_loss_43/value:0\", shape=(), dtype=float32)\n",
      "epoch : 6, loss: Tensor(\"sparse_softmax_cross_entropy_loss_44/value:0\", shape=(), dtype=float32)\n",
      "epoch : 7, loss: Tensor(\"sparse_softmax_cross_entropy_loss_45/value:0\", shape=(), dtype=float32)\n",
      "epoch : 8, loss: Tensor(\"sparse_softmax_cross_entropy_loss_46/value:0\", shape=(), dtype=float32)\n",
      "epoch : 9, loss: Tensor(\"sparse_softmax_cross_entropy_loss_47/value:0\", shape=(), dtype=float32)\n",
      "epoch : 10, loss: Tensor(\"sparse_softmax_cross_entropy_loss_48/value:0\", shape=(), dtype=float32)\n",
      "epoch : 11, loss: Tensor(\"sparse_softmax_cross_entropy_loss_49/value:0\", shape=(), dtype=float32)\n",
      "epoch : 12, loss: Tensor(\"sparse_softmax_cross_entropy_loss_50/value:0\", shape=(), dtype=float32)\n",
      "epoch : 13, loss: Tensor(\"sparse_softmax_cross_entropy_loss_51/value:0\", shape=(), dtype=float32)\n",
      "epoch : 14, loss: Tensor(\"sparse_softmax_cross_entropy_loss_52/value:0\", shape=(), dtype=float32)\n",
      "epoch : 15, loss: Tensor(\"sparse_softmax_cross_entropy_loss_53/value:0\", shape=(), dtype=float32)\n",
      "epoch : 16, loss: Tensor(\"sparse_softmax_cross_entropy_loss_54/value:0\", shape=(), dtype=float32)\n",
      "epoch : 17, loss: Tensor(\"sparse_softmax_cross_entropy_loss_55/value:0\", shape=(), dtype=float32)\n",
      "epoch : 18, loss: Tensor(\"sparse_softmax_cross_entropy_loss_56/value:0\", shape=(), dtype=float32)\n",
      "epoch : 19, loss: Tensor(\"sparse_softmax_cross_entropy_loss_57/value:0\", shape=(), dtype=float32)\n",
      "epoch : 20, loss: Tensor(\"sparse_softmax_cross_entropy_loss_58/value:0\", shape=(), dtype=float32)\n",
      "epoch : 21, loss: Tensor(\"sparse_softmax_cross_entropy_loss_59/value:0\", shape=(), dtype=float32)\n",
      "epoch : 22, loss: Tensor(\"sparse_softmax_cross_entropy_loss_60/value:0\", shape=(), dtype=float32)\n",
      "epoch : 23, loss: Tensor(\"sparse_softmax_cross_entropy_loss_61/value:0\", shape=(), dtype=float32)\n",
      "epoch : 24, loss: Tensor(\"sparse_softmax_cross_entropy_loss_62/value:0\", shape=(), dtype=float32)\n",
      "epoch : 25, loss: Tensor(\"sparse_softmax_cross_entropy_loss_63/value:0\", shape=(), dtype=float32)\n",
      "epoch : 26, loss: Tensor(\"sparse_softmax_cross_entropy_loss_64/value:0\", shape=(), dtype=float32)\n",
      "epoch : 27, loss: Tensor(\"sparse_softmax_cross_entropy_loss_65/value:0\", shape=(), dtype=float32)\n",
      "epoch : 28, loss: Tensor(\"sparse_softmax_cross_entropy_loss_66/value:0\", shape=(), dtype=float32)\n",
      "epoch : 29, loss: Tensor(\"sparse_softmax_cross_entropy_loss_67/value:0\", shape=(), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(30):\n",
    "    \n",
    "    # Inicializar los estados a cero\n",
    "    hidden= modelo.reset_states() \n",
    "    \n",
    "    # Datos\n",
    "    X, output=data.get_batch(seq_length,batch_size)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, hidden= modelo(X, hidden)\n",
    "        \n",
    "        output= tf.reshape(output,(-1,) )\n",
    "        loss= tf.losses.sparse_softmax_cross_entropy(labels=output, logits=tf.transpose(predictions))\n",
    "    \n",
    "    # back prop \n",
    "    grads= tape.gradient(loss, modelo.variables)\n",
    "    optimizer.apply_gradients(zip(grads, modelo.variables))\n",
    "    print(f'epoch : {epoch}, loss: {loss}')\n",
    "        \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "foo= tf.ones([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'float32'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "foo.dtype.base_dtype.name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'ones:0' shape=(1,) dtype=float32>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
