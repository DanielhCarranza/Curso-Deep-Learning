{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oOvEZ5ipdegh"
   },
   "source": [
    "# Recurrent Neural Networks\n",
    "\n",
    "Recurrent neural networks, or RNNs, are a family of neural networks for processing sequential data. Much as a convolutional networkis a neural network that is specialized for processing a grid of values Xsuch as an image, a recurrent neural network is a neural network that is specialized for processing a sequence of values $ x^{(1)} ,...x^{(T)}$ \n",
    "![alt text](https://stanford.edu/~shervine/images/architecture-rnn.png)\n",
    "\n",
    "* $x^{<t>}$ is the input at time step t. For example, $x^{<1>}$ could be a one-hot vector corresponding to the second word of a sentence.\n",
    "* $a^{<t>}$ is the hidden state at time step t. It’s the “memory” of the network. $a^{<t>}$ is calculated based on the previous hidden state and the input at the current step: $a^{<t>}=f(Ux_t + Wa_{t-1})$. The function f usually is a nonlinearity such as tanh or ReLU.  $a^{<0>}$, which is required to calculate the first hidden state, is typically initialized to all zeros.\n",
    "* $y^{<t>}$ is the output at step t. For example, if we wanted to predict the next word in a sentence it would be a vector of probabilities across our vocabulary. $y^{<t>} = \\mathrm{softmax}(Vy_t)$.\n",
    "\n",
    "For each timestep $t$ the activation $a^{<t>}$  and the output $y^{<t>}$ \n",
    "$\\boxed{a^{< t >}=g_1(W_{aa}a^{< t-1 >}+W_{ax}x^{< t >}+b_a)}\\quad\\mbox{and}\\quad\\boxed{y^{< t >}=g_2(W_{ya}a^{< t >}+b_y)}$\n",
    "\n",
    "where $W_{ax}, W_{aa}, W_{ya}, b_a, b_y$ are coefficients that are shared temporally and $g_1, g_2$  activations functions\n",
    "\n",
    "![alt text](https://stanford.edu/~shervine/images/description-block-rnn.png)\n",
    "\n",
    "The pros and cons of a typical RNN architecture are summed up in the table below:\n",
    "\n",
    "## Advantages \n",
    "*  Possibility of processing input of any length\n",
    "*  Model size not increasing with size of input\n",
    "*  Computation takes into account historical information\n",
    "*  Weights are shared across time\n",
    "\n",
    "## Drawbacks\n",
    "* Computation being slow\n",
    "* Difficulty of accessing information from a long time ago\n",
    "* Cannot consider any future input for the current state\n",
    "\n",
    "\n",
    "### One to One \n",
    "Tradicional Neural Network \n",
    "$$T_x=T_y=1$$\n",
    "![alt text](https://stanford.edu/~shervine/images/rnn-one-to-one.png)\n",
    "\n",
    "### One to Many\n",
    "Music generation\n",
    "\n",
    "$$T_x=1, T_y>1$$\n",
    "![alt text](https://stanford.edu/~shervine/images/rnn-one-to-many.png)\n",
    "\n",
    "### Many to One\n",
    "Sentiment Clasification\n",
    "$$T_x>1, T_y=1$$\n",
    "![alt text](https://stanford.edu/~shervine/images/rnn-many-to-one.png)\n",
    "\n",
    "\n",
    "### Many to Many \n",
    "Name Entity Recognition\n",
    "$$T_x=T_y$$\n",
    "![alt text](https://stanford.edu/~shervine/images/rnn-many-to-many-same.png)\n",
    "\n",
    "### Many to Many\n",
    "Machine Translation\n",
    "$$T_x\\neq T_y$$\n",
    "![alt text](https://stanford.edu/~shervine/images/rnn-many-to-many-different.png)\n",
    "\n",
    "**Loss function** ― In the case of a recurrent neural network, the loss function  $L$ of all time steps is defined based on the loss at every time step as follows:\n",
    "$$\\boxed{\\mathcal{L}(\\widehat{y},y)=\\sum_{t=1}^{T_y}\\mathcal{L}(\\widehat{y}^{< t >},y^{< t >})}$$\n",
    "\n",
    "**Backpropagation through time** ― Backpropagation is done at each point in time. At timestep $T$, the derivative of the loss $L$ with respect to weight matrix $W$is expressed as follows:\n",
    "$$ \\boxed{\\frac{\\partial \\mathcal{L}^{(T)}}{\\partial W}=\\sum_{t=1}^T\\left.\\frac{\\partial\\mathcal{L}^{(T)}}{\\partial W}\\right|_{(t)}}$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rukh9sX3OP_r"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.eager.python import tfe\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H-A1FdsEOP_y"
   },
   "outputs": [],
   "source": [
    "# enable eager mode\n",
    "tf.enable_eager_execution()\n",
    "tf.set_random_seed(0)\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_D3dXNRcOP_2"
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('weights/'):\n",
    "    os.makedirs('weights/')\n",
    "\n",
    "# constants\n",
    "units = 128\n",
    "batch_size = 100\n",
    "epochs = 2\n",
    "num_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IvbNEN6CwDZd"
   },
   "outputs": [],
   "source": [
    "\n",
    "class DataLoader():\n",
    "    \"\"\"Load data MNIST \"\"\"\n",
    "    def __init__(self):\n",
    "      \n",
    "        # Download data\n",
    "        (self.X_train, self.y_train),(self.X_test,self.y_test)= tf.keras.datasets.mnist.load_data()\n",
    "        \n",
    "        # Preprocessing\n",
    "        self.X_train = self.X_train.reshape(-1, 28, 28 ).astype(np.float32)/255.0 # Debe ser de la forma [batch, H, W, num_canales]\n",
    "        self.X_test  = self.X_test.reshape(-1, 28, 28).astype(np.float32)/255.0\n",
    "        self.y_train=self.y_train.astype(np.int32) \n",
    "        self.y_test=self.y_test.astype(np.int32)\n",
    "        \n",
    "    def get_batch(self,batch_size):\n",
    "        # Muestreo aleatorio de los datos de la forma [0, stop, size]\n",
    "        index=np.random.randint(0, self.X_train.shape[0], batch_size)\n",
    "        return self.X_train[index,:], self.y_train[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gjZ-KT80wPQz",
    "outputId": "11678080-a04b-4964-de23-ee524e457f28"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60000, 28, 28)"
      ]
     },
     "execution_count": 59,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data=DataLoader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "PbiRmGntOP_5",
    "outputId": "f357514f-8979-4575-e692-fbaa0549a945"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('x train', (60000, 28, 28))\n",
      "('y train', (60000, 10))\n",
      "('x test', (10000, 28, 28))\n",
      "('y test', (10000, 10))\n"
     ]
    }
   ],
   "source": [
    "# onehot encoding \n",
    "y_train_ohe = tf.one_hot(data.y_train, depth=num_classes).numpy()\n",
    "y_test_ohe = tf.one_hot(data.y_test, depth=num_classes).numpy()\n",
    "\n",
    "print('x train', data.X_train.shape)\n",
    "print('y train', y_train_ohe.shape)\n",
    "print('x test', data.X_test.shape)\n",
    "print('y test', y_test_ohe.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0OC6MxkIOP__"
   },
   "source": [
    "# Long Short Term Memory\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn.\n",
    "\n",
    "The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.\n",
    "The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.\n",
    "\n",
    "![alt text](https://stanford.edu/~shervine/images/lstm.png)\n",
    "\n",
    "$$\\tilde{c}^{< t >}=\\textrm{tanh}(W_c[\\Gamma_r\\star a^{< t-1 >},x^{< t >}]+b_c)$$\n",
    "$$c^{< t >}= \\Gamma_u\\star\\tilde{c}^{< t >}+\\Gamma_f\\star c^{< t-1 >}$$\n",
    "$$a^{< t >}=\\Gamma_o\\star c^{< t >}$$\n",
    "\n",
    "The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\n",
    "\n",
    "**Gates** :\n",
    " A system of gating units that controls the ﬂow of information\n",
    "* Update gate $\\Gamma_u$--> How much past should matter now?\n",
    "* Forget Gate $\\Gamma_f$-->Erase a cell or not?\n",
    "* Output gate $ \\Gamma_o$--> How much to reveal of a cell?\n",
    "* Reveleance gate  $ \\Gamma_r$-->  Drop previous information?\n",
    "\n",
    "[Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4Tl1k4NfOxax"
   },
   "outputs": [],
   "source": [
    "class BasicLSTM(tf.keras.Model):\n",
    "    def __init__(self, units, return_sequence=False, return_states=False, **kwargs):\n",
    "        super(BasicLSTM, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.return_sequence = return_sequence\n",
    "        self.return_states = return_states\n",
    "\n",
    "        def bias_initializer(_, *args, **kwargs):\n",
    "            return tf.keras.backend.concatenate([\n",
    "                tf.keras.initializers.Zeros()((self.units,), *args, **kwargs),  # input gate\n",
    "                tf.keras.initializers.Ones()((self.units,), *args, **kwargs),  # forget gate\n",
    "                tf.keras.initializers.Zeros()((self.units * 2,), *args, **kwargs),  # context and output gates\n",
    "            ])\n",
    "\n",
    "        self.kernel = tf.keras.layers.Dense(4 * units, use_bias=False)\n",
    "        self.recurrent_kernel = tf.keras.layers.Dense(4 * units, kernel_initializer='glorot_uniform', bias_initializer=bias_initializer)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None, initial_states=None):\n",
    "        # reset the states initially if not provided, else use those\n",
    "        if initial_states is None:\n",
    "            h_state = tf.zeros((inputs.shape[0], self.units))\n",
    "            c_state = tf.zeros((inputs.shape[0], self.units))\n",
    "        else:\n",
    "            assert len(initial_states) == 2, \"Must pass a list of 2 states when passing 'initial_states'\"\n",
    "            h_state, c_state = initial_states\n",
    "\n",
    "        h_list = []\n",
    "        c_list = []\n",
    "\n",
    "        for t in range(inputs.shape[1]):\n",
    "            # LSTM gate steps\n",
    "            ip = inputs[:, t, :]\n",
    "            z = self.kernel(ip)\n",
    "            z += self.recurrent_kernel(h_state)\n",
    "\n",
    "            z0 = z[:, :self.units]\n",
    "            z1 = z[:, self.units: 2 * self.units]\n",
    "            z2 = z[:, 2 * self.units: 3 * self.units]\n",
    "            z3 = z[:, 3 * self.units:]\n",
    "\n",
    "            # gate updates\n",
    "            i = tf.keras.activations.sigmoid(z0)\n",
    "            f = tf.keras.activations.sigmoid(z1)\n",
    "            c = f * c_state + i * tf.nn.tanh(z2)\n",
    "\n",
    "            # state updates\n",
    "            o = tf.keras.activations.sigmoid(z3)\n",
    "            h = o * tf.nn.tanh(c)\n",
    "\n",
    "            h_state = h\n",
    "            c_state = c\n",
    "\n",
    "            h_list.append(h_state)\n",
    "            c_list.append(c_state)\n",
    "\n",
    "        hidden_outputs = tf.stack(h_list, axis=1)\n",
    "        hidden_states = tf.stack(c_list, axis=1)\n",
    "\n",
    "        if self.return_states and self.return_sequence:\n",
    "            return hidden_outputs, [hidden_outputs, hidden_states]\n",
    "        elif self.return_states and not self.return_sequence:\n",
    "            return hidden_outputs[:, -1, :], [h_state, c_state]\n",
    "        elif self.return_sequence and not self.return_states:\n",
    "            return hidden_outputs\n",
    "        else:\n",
    "            return hidden_outputs[:, -1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8x4MSHPHOQAA"
   },
   "outputs": [],
   "source": [
    "class BasicLSTMModel(tf.keras.Model):\n",
    "    def __init__(self, units, num_classes):\n",
    "        super(BasicLSTMModel, self).__init__()\n",
    "        self.units = units\n",
    "        self.lstm = BasicLSTM(units)\n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        h = self.lstm(inputs)\n",
    "        output = self.classifier(h)\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "UKcN0nkwOQAE",
    "outputId": "dd8a1256-ebe2-4f94-8015-6dcdcee69bff"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "600/600 [==============================] - 104s 173ms/step - loss: 0.3358 - acc: 0.8908 - val_loss: 0.1187 - val_acc: 0.9632\n",
      "Epoch 2/2\n",
      "600/600 [==============================] - 103s 171ms/step - loss: 0.0967 - acc: 0.9722 - val_loss: 0.1014 - val_acc: 0.9720\n",
      "100/100 [==============================] - 5s 53ms/step\n",
      "('Final test loss and accuracy :', [0.1013777525583282, 0.9720000064373017])\n"
     ]
    }
   ],
   "source": [
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "  \n",
    "    # build model and optimizer\n",
    "    model = BasicLSTMModel(units, num_classes)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    dummy_x = tf.zeros((1, 28, 28))\n",
    "    model._set_inputs(dummy_x)\n",
    "\n",
    "    # train\n",
    "    model.fit(data.X_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(data.X_test, y_test_ohe), verbose=1)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
    "    print(\"Final test loss and accuracy :\", scores)\n",
    "\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.save('weights/06_02_rnn/weights.ckpt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "dsPumfYmaIIh"
   },
   "source": [
    "### LSTM cell pre-built"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gjZy5OHOvVLS"
   },
   "outputs": [],
   "source": [
    "class LSTM(tf.keras.Model):\n",
    "    def __init__(self, units, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.units = units\n",
    "        self.lstm_cell = tf.nn.rnn_cell.LSTMCell(units)  \n",
    "        self.classifier = tf.keras.layers.Dense(num_classes)\n",
    "\n",
    "    def call(self, inputs, training=None, mask=None):\n",
    "        state = self.lstm_cell.zero_state(batch_size=inputs.shape[0], dtype=tf.float32)\n",
    "        x = inputs\n",
    "\n",
    "        for t in range(inputs.shape[1]):\n",
    "            input = inputs[:, t, :]  \n",
    "            x, state = self.lstm_cell(input, state=state) \n",
    "\n",
    "        output = self.classifier(x)  # feed the last `x` as the hidden embedding of the lstm to the classifier\n",
    "\n",
    "        # softmax op does not exist on the gpu, so always use cpu\n",
    "        with tf.device('/cpu:0'):\n",
    "            output = tf.nn.softmax(output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "cqOz3eGTvVXB"
   },
   "outputs": [],
   "source": [
    "\n",
    "device = '/cpu:0' if tfe.num_gpus() == 0 else '/gpu:0'\n",
    "\n",
    "with tf.device(device):\n",
    "  \n",
    "    # build model and optimizer\n",
    "    model = LSTM(units, num_classes)\n",
    "    model.compile(optimizer=tf.train.AdamOptimizer(0.01), loss='categorical_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    dummy_x = tf.zeros((1, 28, 28))\n",
    "    model._set_inputs(dummy_x)\n",
    "\n",
    "    # train\n",
    "    model.fit(x_train, y_train_ohe, batch_size=batch_size, epochs=epochs,\n",
    "              validation_data=(x_test, y_test_ohe), verbose=1)\n",
    "\n",
    "    # evaluate on test set\n",
    "    scores = model.evaluate(x_test, y_test_ohe, batch_size, verbose=1)\n",
    "    print(\"Final test loss and accuracy :\", scores)\n",
    "\n",
    "    saver = tfe.Saver(model.variables)\n",
    "    saver.save('weights/06_03_rnn/weights.ckpt')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM with Tensorflow.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
