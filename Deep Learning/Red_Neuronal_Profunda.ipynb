{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gedLu2JxtRi4"
   },
   "source": [
    "# Red Neuronal Profunda\n",
    "\n",
    "\n",
    "**Notacion**:\n",
    "- Superscript $[l]$ denota la cantidad  asociada con la capa $l^{th}$ . \n",
    "    - Ejemplo: $a^{[L]}$ es la $L^{th}$ capa de activacion. $W^{[L]}$ y $b^{[L]}$ son los parametros de la capa $L^{th}$ .\n",
    "- Superscript $(i)$ denota la cantidad asociada con el  $i^{th}$ ejemplo. \n",
    "    - Ejemplo: $x^{(i)}$ es el $i^{th}$ ejemplor de entrenamiento.\n",
    "- Lowerscript $i$ denota la $i^{th}$ entrada del vector.\n",
    "    - Ejemplo: $a^{[l]}_i$ denota ls $i^{th}$ entrada a la capa de acticacion $l^{th}$ \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ObiQdt9rtRi6"
   },
   "source": [
    "## 1 - Importar modulos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eySh3TyJtRi7"
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-CaJYMOGYzAi"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DodBF-OwkRDJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4AOixTRkkWhZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LT-B0DkRtRjE"
   },
   "source": [
    "## 2 - Inicializacion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Y7wfMa31tRjP"
   },
   "source": [
    "### 2.1 - L-layer Neural Network\n",
    "\n",
    "construiremos una funcion para $L$ capas \n",
    " Recuerda que  $n^{[l]}$ es el  numero de unidades de la capa  $l$. Por ejepmplo si el tamaño de la entrada es  $X$ con forma $(12288, 209)$ (donde $m=209$ ejemplos) entonces:\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "\n",
    "\n",
    "    <tr>\n",
    "        <td>  </td> \n",
    "        <td> **Shape of W** </td> \n",
    "        <td> **Shape of b**  </td> \n",
    "        <td> **Activation** </td>\n",
    "        <td> **Shape of Activation** </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 1** </td> \n",
    "        <td> $(n^{[1]},12288)$ </td> \n",
    "        <td> $(n^{[1]},1)$ </td> \n",
    "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
    "        \n",
    "        <td> $(n^{[1]},209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    <tr>\n",
    "        <td> **Layer 2** </td> \n",
    "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
    "        <td> $(n^{[2]},1)$ </td> \n",
    "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
    "        <td> $(n^{[2]}, 209)$ </td> \n",
    "    <tr>\n",
    "   \n",
    "       <tr>\n",
    "        <td> $\\vdots$ </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$  </td> \n",
    "        <td> $\\vdots$</td> \n",
    "        <td> $\\vdots$  </td> \n",
    "    <tr>\n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L-1** </td> \n",
    "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
    "        <td> $(n^{[L-1]}, 1)$  </td> \n",
    "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
    "        <td> $(n^{[L-1]}, 209)$ </td> \n",
    "    <tr>\n",
    "    \n",
    "    \n",
    "   <tr>\n",
    "        <td> **Layer L** </td> \n",
    "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
    "        <td> $(n^{[L]}, 1)$ </td>\n",
    "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
    "        <td> $(n^{[L]}, 209)$  </td> \n",
    "    <tr>\n",
    "\n",
    "</table>\n",
    "\n",
    "Recuerda que cuando computas  $W X + b$ en python, hace el broadcasting. Por ejemplo, si: \n",
    "\n",
    "$$ W = \\begin{bmatrix}\n",
    "    j  & k  & l\\\\\n",
    "    m  & n & o \\\\\n",
    "    p  & q & r \n",
    "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
    "    a  & b  & c\\\\\n",
    "    d  & e & f \\\\\n",
    "    g  & h & i \n",
    "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
    "    s  \\\\\n",
    "    t  \\\\\n",
    "    u\n",
    "\\end{bmatrix}\\tag{2}$$\n",
    "\n",
    "Entonces  $WX + b$ sera:\n",
    "\n",
    "$$ WX + b = \\begin{bmatrix}\n",
    "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
    "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
    "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
    "\\end{bmatrix}\\tag{3}  $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "D3xzDgcptRjR"
   },
   "source": [
    "\n",
    "**Instruciones**:\n",
    "- La estructura del modelo es:   *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e., que tiene  $L-1$ capas usando a ReLU activation seguida de un sigmoide\n",
    "- Usa random initialization para las matrices weight. \n",
    "- Usa zeros initialization para los  biases\n",
    "- Nosotros guardaremos el valor  $n^{[l]}$, que es el numero de las unidades en las diferentes capas , en  la variable `layer_dims`. Por example, una `layer_dims` de forma [2,4,1]: Eso significa que tenemos dos entradas, una hidden layer con 4 hidden units, y una output layer una  unidad ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "w2mwABhstRjR"
   },
   "outputs": [],
   "source": [
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    Argumentos:\n",
    "    layer_dims -- python array (list) que contiene las dimensiones de nuestra red neuronal \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python diccionario que contiene los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl -- weight matrix de la forma (layer_dims[l], layer_dims[l-1])\n",
    "                    bl -- bias vector de la forma (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "        \n",
    "    return parametros"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ev5-jFlytRjU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XDAFYRL-tRjX"
   },
   "source": [
    "## 3 - Forward propagation modulo\n",
    "\n",
    "### 3.1 - Linear Forward \n",
    "Modulo Linear Foward:\n",
    "\n",
    "- LINEAR\n",
    "- LINEAR -> ACTIVATION donde  ACTIVATION puede ser ReLU o Sigmoid. \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID \n",
    "\n",
    "La funcion vectorizada para computar el Foward prop modulo sobre todos los ejemplos es:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
    "\n",
    "donde $A^{[0]} = X$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "khkbmfnGtRjY"
   },
   "outputs": [],
   "source": [
    "def linear_forward(A, W, b):\n",
    "    \"\"\"\n",
    "    Implementa el modulo de forward propagation para Lth capas.\n",
    "\n",
    "    Argumentos:\n",
    "    A -- activaciones de la capa anterior(o datos de entrada): (Tamaño de la capa anterior, numero de ejemplos)\n",
    "    W -- weights matrix: numpy array de la forma (Tamaño de la capa actual, Tamaño de la capa anterior)\n",
    "    b -- bias vector, numpy array de la forma (Tamaño de la capa actual, 1)\n",
    "\n",
    "    Returns:\n",
    "    Z -- la entrada de la activacion \n",
    "    cache --  \"A\", \"W\" y \"b\" \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "IE_IsZdtjvpV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9PeECgzDtRjf"
   },
   "source": [
    "### 3.2 - Linear-Activation Forward\n",
    "\n",
    "Funciones de activacion\n",
    "\n",
    "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
    "``` python\n",
    "A, activation_cache = sigmoid(Z)\n",
    "```\n",
    "\n",
    "- **ReLU**: La formula matematica  para ReLu es $A = RELU(Z) = max(0, Z)$. \n",
    "``` python\n",
    "A, activation_cache = relu(Z)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tpqh7WghtRjg"
   },
   "source": [
    "Implementar el paso de  forward propagation para la capa  *LINEAR->ACTIVATION* . \n",
    "\n",
    "La relacion matematica es la siguiente: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ donde   \"g\"  es la fn activacion que puede ser ya sea sigmoid() o relu(). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qMoZo7VttRjg"
   },
   "outputs": [],
   "source": [
    "\n",
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Implementar el paso de  forward propagation para la capa LINEAR->ACTIVATION \n",
    "\n",
    "    Argumentos:\n",
    "    A_prev -- activaciones de la capa anterior(o datos de entrada): (Tamaño de la capa anterior, numero de ejemplos)\n",
    "    W -- weights matrix: numpy array de la forma (Tamaño de la capa actual, Tamaño de la capa anterior)\n",
    "    b -- bias vector, numpy array de la forma (Tamaño de la capa actual, 1)\n",
    "    activation -- la fn de  activacion que sera usada en esta capa\n",
    "    \n",
    "    \n",
    "    Returns:\n",
    "    A -- post-activacion \n",
    "    cache --  diccionario \"linear_cache\" y \"activation_cache\";\n",
    "             \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kT1OgS00tRjo"
   },
   "source": [
    "### 3.3 Modelo de L capas\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "88eb9kQEtRjw"
   },
   "outputs": [],
   "source": [
    "def L_model_forward(X, parametros):\n",
    "    \"\"\"\n",
    "    Implementa forward propagation para [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID \n",
    "    \n",
    "    Arguments:\n",
    "    X -- datos, numpy array de shape (Tamaño de entrada, numero de ejemplos)\n",
    "    parametros -- salida de  initialize_parameters_deep()\n",
    "    \n",
    "    Returns:\n",
    "    AL --  valor de post-activacion \n",
    "    caches \n",
    "    \"\"\"\n",
    "\n",
    "\n",
    "            \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mISbTkYMtRj2"
   },
   "source": [
    "## 4 - Cost function\n",
    "\n",
    "\n",
    "**Crea la funcion**: Categorical cross-entropy cost \n",
    "$$J=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9DuTcXaqtRj4"
   },
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Implementa la cost function  (7).\n",
    "\n",
    "    Argumentoss:\n",
    "    AL -- vector de probabilidad correspondinte a las predicciones, shape (1, numero de ejemplos)\n",
    "    Y -- vector de  clases o categorias a predecir, shape (1, numero de ejemplos)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wLuT8r6LtRj9"
   },
   "source": [
    "## 5 - Backward propagation modulo\n",
    "\n",
    "\n",
    "\n",
    "**Recordatorio sobre backprop** \n",
    "Recuerda que para encontrar los gradientes tienes que computar la derivada parcial $\\mathcal{L}$ con respecto a los parametros por ejemplo $z^{[1]}$ para una rnn de dos capas:\n",
    "\n",
    "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
    "\n",
    "\n",
    "\n",
    "Ahora computaremos el modulo de backprop en tres pasos:\n",
    "- LINEAR backward\n",
    "- LINEAR -> ACTIVATION backward donde ACTIVATION computa la derivada de  ReLU o sigmoid \n",
    "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pCbFW-EMtRj-"
   },
   "source": [
    "### 5.1 - Linear backward\n",
    "\n",
    "Formulas vectorizada para el modulo de bp\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e6kaKIaEtRkE"
   },
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implementa el modulo para  backward propagation para una capa\n",
    "\n",
    "    Argumentos:\n",
    "    dZ -- Gradiente del cost con respecto a la capa actual \n",
    "    cache -- tupla de los valores (A_prev, W, b) proveninetes del  forward propagation en la capa actual\n",
    "\n",
    "    Returns:\n",
    "    dA_prev -- Gradiente del cost con respecto a la activacion\n",
    "    dW -- Gradiente del  cost con respecto a W (capa actual l)\n",
    "    db -- Gradiente del  cost con respecto a b (capa actual l)\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RumwjG6ktRkI"
   },
   "source": [
    "### 5.2 - Linear-Activation backward\n",
    "\n",
    "\n",
    "\n",
    "Si $g(.)$ es la  fn activacion \n",
    "`sigmoid_backward` y `relu_backward` computan  la derivada de la fn actvivacion $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
    "\n",
    "Implementa la capa de  backpropagation  *LINEAR->ACTIVATION* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hgUSHulqtRkJ"
   },
   "outputs": [],
   "source": [
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implementa la capa de backward propagation para LINEAR->ACTIVATION\n",
    "    \n",
    "    Argumentos:\n",
    "    dA -- gradiente de post-activacion de l capa actual l \n",
    "    cache -- tuple de valores (linear_cache, activation_cache) \n",
    "    activation -- fn activacion\n",
    "    \n",
    "    Returns:\n",
    "    dA_prev -- Gradient del cost con respecto a la fn de activacion (de la capa anterior l-1)\n",
    "    dW -- Gradiente del  cost con respecto a W (capa actual l)\n",
    "    db -- Gradiente del  cost con respecto a b (capa actual l)\n",
    "    \"\"\"\n",
    "\n",
    " \n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lSn-f6o9tRkP"
   },
   "source": [
    "### 5.3 - L-Modelo Backward \n",
    "\n",
    "\n",
    "** Inicializacion de backpropagation**:\n",
    "Para hacer backprop atraves de la rednn, sabemos que la salida es, \n",
    "$A^{[L]} = \\sigma(Z^{[L]})$. Y la derivada de su funcion de error con tespecto a A  es : $dA^{[L]}= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
    "\n",
    "donde $dA^{[L]}= \\frac YA + \\frac{(1-Y)}{(1-A)}$\n",
    "\n",
    "una forma de hacerlo con numpy es :\n",
    "\n",
    "```python\n",
    "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3bieU7pHtRkQ"
   },
   "outputs": [],
   "source": [
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implementa el modulo de backward propagation para  [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
    "    \n",
    "    Argumentos:\n",
    "    AL -- vector de probabilidad, salida del forward propagation (L_model_forward())\n",
    "    Y -- clases \n",
    "    caches -- \n",
    "    \n",
    "    Returns:\n",
    "    grads -- Un diccionario de los gradientes\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "      \n",
    "\n",
    "    return grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SFwZVaFAtRkZ"
   },
   "source": [
    "## 6  - Algoritmo de optimizacion \n",
    "\n",
    "Gradient descent: \n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
    "\n",
    "donde  $\\alpha$ es el  learning rate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QOWS38xjtRka"
   },
   "outputs": [],
   "source": [
    "def gradient_descent(parametros, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Gradient descent\n",
    "    \n",
    "    Argumentos:\n",
    "    parametros  \n",
    "    grads  \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- actualizados \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wyZx1h4otRkg"
   },
   "source": [
    "## 7 Prediccion\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H4X3feqp8yP9"
   },
   "outputs": [],
   "source": [
    "def predict(X, y, parametros):\n",
    "    \"\"\"\n",
    "    .\n",
    "    \n",
    "    Argumentos:\n",
    "    X -- data set \n",
    "    parametros\n",
    "    \n",
    "    Returns:\n",
    "    p -- predicciones del dataset dado X\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "        \n",
    "    return p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mefIcTtE8xTw"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1rP504iM8xa_"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Red_Neuronal_Profunda.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
