{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Red_Neuronal_Profunda.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "gedLu2JxtRi4"
      },
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal Profunda\n",
        "\n",
        "\n",
        "**Notacion**:\n",
        "- Superscript $[l]$ denota la cantidad  asociada con la capa $l^{th}$ . \n",
        "    - Ejemplo: $a^{[L]}$ es la $L^{th}$ capa de activacion. $W^{[L]}$ y $b^{[L]}$ son los parametros de la capa $L^{th}$ .\n",
        "- Superscript $(i)$ denota la cantidad asociada con el  $i^{th}$ ejemplo. \n",
        "    - Ejemplo: $x^{(i)}$ es el $i^{th}$ ejemplor de entrenamiento.\n",
        "- Lowerscript $i$ denota la $i^{th}$ entrada del vector.\n",
        "    - Ejemplo: $a^{[l]}_i$ denota ls $i^{th}$ entrada a la capa de acticacion $l^{th}$ \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ObiQdt9rtRi6"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Importar modulos\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eySh3TyJtRi7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np \n",
        "import pandas as pd \n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "\n",
        "\n",
        "% matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wtNDi7N6eG-R",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Breast Cancer Dataset "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-CaJYMOGYzAi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "95aed8c0-ecf6-4b6f-9e11-8faaafd1b72c"
      },
      "cell_type": "code",
      "source": [
        "cancer=load_breast_cancer()\n",
        "X=cancer.data\n",
        "Y=cancer.target\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train,X_test, y_train,y_test= train_test_split(X,Y,test_size=0.2, shuffle =True)\n",
        "\n",
        "print(f'X_train shape: {X_train.shape}')\n",
        "print(f'X_test shape: {X_test.shape}')\n",
        "\n",
        "y_train=y_train.reshape(-1,1)\n",
        "y_test=y_test.reshape(-1,1)\n",
        "\n",
        "print(f'y_train shape: {y_train.shape}')\n",
        "print(f'y_test shape: {y_test.shape}')\n",
        "\n",
        "y_train=y_train.reshape(-1,1)\n"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X_train shape: (455, 30)\n",
            "X_test shape: (114, 30)\n",
            "y_train shape: (455, 1)\n",
            "y_test shape: (114, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "UeB3Ju5KeCQ7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Mnist Dataset"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DodBF-OwkRDJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "(X_train,y_train),(X_test, y_test) = mnist.load_data()\n",
        "\n",
        "X_train= X_train.reshape(-1, 28*28)\n",
        "y_train=y_train.reshape(-1,1)\n",
        "\n",
        "def one_hot_encoding(Y):\n",
        "  n_classes=len(np.unique(Y))\n",
        "  encoding= np.eye(n_classes)[Y.reshape(-1)]\n",
        "  return encoding\n",
        "\n",
        "Y= one_hot_encoding(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LT-B0DkRtRjE"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Inicializacion\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y7wfMa31tRjP"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 - L-layer Neural Network\n",
        "\n",
        "construiremos una funcion para $L$ capas \n",
        " Recuerda que  $n^{[l]}$ es el  numero de unidades de la capa  $l$. Por ejepmplo si el tama√±o de la entrada es  $X$ con forma $(12288, 209)$ (donde $m=209$ ejemplos) entonces:\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "\n",
        "    <tr>\n",
        "        <td>  </td> \n",
        "        <td> **Shape of W** </td> \n",
        "        <td> **Shape of b**  </td> \n",
        "        <td> **Activation** </td>\n",
        "        <td> **Shape of Activation** </td> \n",
        "    </tr>\n",
        "\n",
        "    <tr>\n",
        "        <td> **Layer 1** </td> \n",
        "        <td> $(n^{[1]},12288)$ </td> \n",
        "        <td> $(n^{[1]},1)$ </td> \n",
        "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
        "        <td> $(n^{[1]},209)$ </td> \n",
        "    </tr>\n",
        "\n",
        "    <tr>\n",
        "        <td> **Layer 2** </td> \n",
        "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
        "        <td> $(n^{[2]},1)$ </td> \n",
        "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
        "        <td> $(n^{[2]}, 209)$ </td> \n",
        "    </tr>\n",
        "\n",
        "    <tr>\n",
        "        <td> $\\vdots$ </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$</td> \n",
        "        <td> $\\vdots$  </td> \n",
        "    </tr>\n",
        "\n",
        "    <tr>\n",
        "        <td> **Layer L-1** </td> \n",
        "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
        "        <td> $(n^{[L-1]}, 1)$  </td> \n",
        "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
        "        <td> $(n^{[L-1]}, 209)$ </td> \n",
        "    </tr>\n",
        "\n",
        "\n",
        "    <tr>\n",
        "        <td> **Layer L** </td> \n",
        "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
        "        <td> $(n^{[L]}, 1)$ </td>\n",
        "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
        "        <td> $(n^{[L]}, 209)$  </td> \n",
        "    </tr>\n",
        "\n",
        "</table>\n",
        "\n",
        "Recuerda que cuando computas  $W X + b$ en python, hace el broadcasting. Por ejemplo, si: \n",
        "\n",
        "$$ W = \\begin{bmatrix}\n",
        "    j  & k  & l\\\\\n",
        "    m  & n & o \\\\\n",
        "    p  & q & r \n",
        "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
        "    a  & b  & c\\\\\n",
        "    d  & e & f \\\\\n",
        "    g  & h & i \n",
        "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
        "    s  \\\\\n",
        "    t  \\\\\n",
        "    u\n",
        "\\end{bmatrix}\\tag{2}$$\n",
        "\n",
        "Entonces  $WX + b$ sera:\n",
        "\n",
        "$$ WX + b = \\begin{bmatrix}\n",
        "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
        "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
        "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
        "\\end{bmatrix}\\tag{3}  $$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D3xzDgcptRjR"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Instruciones**:\n",
        "- La estructura del modelo es:   *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e., que tiene  $L-1$ capas usando a ReLU activation seguida de un sigmoide\n",
        "- Usa random initialization para las matrices weight. \n",
        "- Usa zeros initialization para los  biases\n",
        "- Nosotros guardaremos el valor  $n^{[l]}$, que es el numero de las unidades en las diferentes capas , en  la variable `layer_dims`. Por example, una `layer_dims` de forma [2,4,1]: Eso significa que tenemos dos entradas, una hidden layer con 4 hidden units, y una output layer una  unidad ."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w2mwABhstRjR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Argumentos:\n",
        "    layer_dims -- python array (list) que contiene las dimensiones de nuestra red neuronal\n",
        "    p.e [In, 100, 100,50,10]\n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python diccionario que contiene los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix de la forma (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector de la forma (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    np.random.seed(3)\n",
        "    parametros={}\n",
        "    L =len(layer_dims) # numero de capas\n",
        "    \n",
        "    for l in range(1, L):\n",
        "        parametros['W' + str(l)] = np.random.randn(layer_dims[l], layer_dims[l-1])*0.01 # Forma W: (N[l], N[l-1])\n",
        "        parametros['b' + str(l)] = np.zeros((layer_dims[l],1)) # Forma Bias : (N[l], 1)\n",
        "        \n",
        "    return parametros"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ev5-jFlytRjU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "7bbddab5-d868-4d1f-8cd1-77e5dde46ded"
      },
      "cell_type": "code",
      "source": [
        "param= initialize_parameters_deep([5,4,3])\n",
        "param"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'W1': array([[ 1.78862847,  0.43650985,  0.09649747, -1.8634927 , -0.2773882 ],\n",
              "        [-0.35475898, -0.08274148, -0.62700068, -0.04381817, -0.47721803],\n",
              "        [-1.31386475,  0.88462238,  0.88131804,  1.70957306,  0.05003364],\n",
              "        [-0.40467741, -0.54535995, -1.54647732,  0.98236743, -1.10106763]]),\n",
              " 'W2': array([[-1.18504653, -0.2056499 ,  1.48614836,  0.23671627],\n",
              "        [-1.02378514, -0.7129932 ,  0.62524497, -0.16051336],\n",
              "        [-0.76883635, -0.23003072,  0.74505627,  1.97611078]]),\n",
              " 'b1': array([[0.],\n",
              "        [0.],\n",
              "        [0.],\n",
              "        [0.]]),\n",
              " 'b2': array([[0.],\n",
              "        [0.],\n",
              "        [0.]])}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XDAFYRL-tRjX"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 - Forward propagation modulo\n",
        "\n",
        "### 3.1 - Linear Forward \n",
        "Modulo Linear Foward:\n",
        "\n",
        "- LINEAR\n",
        "- LINEAR -> ACTIVATION donde  ACTIVATION puede ser ReLU o Sigmoid. \n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID \n",
        "\n",
        "La funcion vectorizada para computar el Foward prop modulo sobre todos los ejemplos es:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "donde $A^{[0]} = X$. \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "khkbmfnGtRjY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implementa el modulo de forward propagation para Lth capas.\n",
        "\n",
        "    Argumentos:\n",
        "    A -- activaciones de la capa anterior(o datos de entrada): (Tama√±o de la capa anterior, numero de ejemplos)\n",
        "    W -- weights matrix: numpy array de la forma (Tama√±o de la capa actual, Tama√±o de la capa anterior)\n",
        "    b -- bias vector, numpy array de la forma (Tama√±o de la capa actual, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- la entrada de la activacion \n",
        "    cache --  \"A\", \"W\" y \"b\" \n",
        "    \"\"\"\n",
        "    # Transformacion lineal\n",
        "    Z =W @ A + b\n",
        "    \n",
        "    cache=(A,W,b)\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "-TSSecrQHry4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "param= initialize_parameters_deep([784,4,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9PeECgzDtRjf"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Linear-Activation Forward\n",
        "\n",
        "Funciones de activacion\n",
        "\n",
        "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
        "``` python\n",
        "A, activation_cache = sigmoid(Z)\n",
        "```\n",
        "\n",
        "- **ReLU**: La formula matematica  para ReLu es $A = RELU(Z) = max(0, Z)$. \n",
        "``` python\n",
        "A, activation_cache = relu(Z)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Tpqh7WghtRjg"
      },
      "cell_type": "markdown",
      "source": [
        "Implementar el paso de  forward propagation para la capa  *LINEAR->ACTIVATION* . \n",
        "\n",
        "La relacion matematica es la siguiente: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ donde   \"g\"  es la fn activacion que puede ser ya sea sigmoid() o relu(). "
      ]
    },
    {
      "metadata": {
        "id": "oiH6KWQ6J025",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def sigmoid(Z):\n",
        "  return 1.0/(1 + np.exp(-np.clip(Z, -250, 250)))\n",
        "\n",
        "def relu(Z):\n",
        "    \"\"\"Funcion de activacion ReLu\"\"\"\n",
        "    A=np.maximum(0,Z)\n",
        "    return A"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qMoZo7VttRjg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implementar el paso de  forward propagation para la capa LINEAR->ACTIVATION \n",
        "\n",
        "    Argumentos:\n",
        "    A_prev -- activaciones de la capa anterior(o datos de entrada): (Tama√±o de la capa anterior, numero de ejemplos)\n",
        "    W -- weights matrix: numpy array de la forma (Tama√±o de la capa actual, Tama√±o de la capa anterior)\n",
        "    b -- bias vector, numpy array de la forma (Tama√±o de la capa actual, 1)\n",
        "    activation -- la fn de  activacion que sera usada en esta capa\n",
        "    \n",
        "    \n",
        "    Returns:\n",
        "    A -- post-activacion \n",
        "    cache --  diccionario \"linear_cache\" y \"activation_cache\";\n",
        "             \n",
        "    \"\"\"\n",
        "    Z, cache= linear_forward(A_prev, W, b)\n",
        "    \n",
        "    if activation == \"sigmoid\":\n",
        "        A = sigmoid(Z)\n",
        "        \n",
        "    else:\n",
        "        A = relu(Z) # Rectifier Linear Unit\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "b7Viz8-5JnTb",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kT1OgS00tRjo"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3 Modelo de L capas\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "88eb9kQEtRjw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parametros):\n",
        "    \"\"\"\n",
        "    Implementa forward propagation para [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID \n",
        "    \n",
        "    Arguments:\n",
        "    X -- datos, numpy array de shape (Tama√±o de entrada, numero de ejemplos)\n",
        "    parametros -- salida de  initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL --  valor de post-activacion \n",
        "    caches \n",
        "    \"\"\"\n",
        "    caches=[]\n",
        "    A=X# X=  A^[0] primera capa\n",
        "    L= len(parametros)//2  # numero de capas o layers\n",
        "    \n",
        "    # Linear --> Relu\n",
        "    for l in range(1,L):\n",
        "        # X\n",
        "        #activation= 'relu ' if l< L else 'sigmoid'\n",
        "        A, cache =linear_activation_forward(A, parametros['W' + str(l)], parametros['b' + str(l)],activation='relu')\n",
        "        # guarda Z ,A , etc que nos sirve para el backward prop\n",
        "        caches.append(cache)\n",
        "\n",
        "    AL, cache = linear_activation_forward(A, parametros[\"W\" + str(L)], parametros[\"b\"+str(L)], activation= 'sigmoid')\n",
        "    caches.append(cache)\n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mISbTkYMtRj2"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 - Cost function\n",
        "\n",
        "\n",
        "**Crea la funcion**: Categorical cross-entropy cost \n",
        "$$J=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))¬†\\tag{7}$$\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9DuTcXaqtRj4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implementa la cost function  (7).\n",
        "\n",
        "    Argumentoss:\n",
        "    AL -- vector de probabilidad correspondinte a las predicciones, shape (1, numero de ejemplos)\n",
        "    Y -- vector de  clases o categorias a predecir, shape (1, numero de ejemplos)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    eps = np.finfo(float).eps\n",
        "    m= Y.shape[0]\n",
        "    loss= -np.sum(Y @ np.log(AL.T + eps) + (1-Y) @ np.log(1-AL.T + eps))/m\n",
        "    \n",
        "    return np.squeeze(loss) "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wLuT8r6LtRj9"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 - Backward propagation modulo\n",
        "\n",
        "\n",
        "\n",
        "**Recordatorio sobre backprop** \n",
        "Recuerda que para encontrar los gradientes tienes que computar la derivada parcial $\\mathcal{L}$ con respecto a los parametros por ejemplo $z^{[1]}$ para una rnn de dos capas:\n",
        "\n",
        "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
        "\n",
        "\n",
        "\n",
        "Ahora computaremos el modulo de backprop en tres pasos:\n",
        "- LINEAR backward\n",
        "- LINEAR -> ACTIVATION backward donde ACTIVATION computa la derivada de  ReLU o sigmoid \n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pCbFW-EMtRj-"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.1 - Linear backward\n",
        "\n",
        "Formulas vectorizada para el modulo de bp\n",
        "\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "e6kaKIaEtRkE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implementa el modulo para  backward propagation para una capa\n",
        "\n",
        "    Argumentos:\n",
        "    dZ -- Gradiente del cost con respecto a la capa actual \n",
        "    cache -- tupla de los valores (A_prev, W, b) proveninetes del  forward propagation en la capa actual\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradiente del cost con respecto a la activacion\n",
        "    dW -- Gradiente del  cost con respecto a W (capa actual l)\n",
        "    db -- Gradiente del  cost con respecto a b (capa actual l)\n",
        "    \"\"\"\n",
        "    A_prev,W,b = cache\n",
        "    m =A_prev.shape[0]\n",
        "    \n",
        "    dW =(dZ @ A_prev.T)/m # misma forma que W\n",
        "    db= np.sum(dZ,axis=1, keepdims=True)/m # misma forma que b\n",
        "    dA_prev =W.T @ dZ # misma forma que A_prev\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RumwjG6ktRkI"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.2 - Linear-Activation backward\n",
        "\n",
        "\n",
        "\n",
        "Si $g(.)$ es la  fn activacion \n",
        "`sigmoid_backward` y `relu_backward` computan  la derivada de la fn actvivacion $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
        "\n",
        "Implementa la capa de  backpropagation  *LINEAR->ACTIVATION* ."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hgUSHulqtRkJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implementa la capa de backward propagation para LINEAR->ACTIVATION\n",
        "    \n",
        "    Argumentos:\n",
        "    dA -- gradiente de post-activacion de l capa actual l \n",
        "    cache -- tuple de valores (linear_cache, activation_cache) \n",
        "    activation -- fn activacion\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient del cost con respecto a la fn de activacion (de la capa anterior l-1)\n",
        "    dW -- Gradiente del  cost con respecto a W (capa actual l)\n",
        "    db -- Gradiente del  cost con respecto a b (capa actual l)\n",
        "    \"\"\"\n",
        "    # Derivada de la funcion de acticacion Relu\n",
        "    if activation =='relu':\n",
        "        # si X>0 ponlo 1 sino 0\n",
        "        dZ = (dA>0)\n",
        "        \n",
        "    # Derivada de la funcion de acticacion sigmoide\n",
        "    elif activation =='sigmoid':\n",
        "        # s=s*(1-s)\n",
        "        s= sigmoid(dA) # sigmoid \n",
        "        dZ= dA*s*(1-s)\n",
        "    \n",
        "    dA_prev,dW,db=linear_backward(dZ,cache)\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lSn-f6o9tRkP"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.3 - L-Modelo Backward \n",
        "\n",
        "\n",
        "** Inicializacion de backpropagation**:\n",
        "Para hacer backprop atraves de la rednn, sabemos que la salida es, \n",
        "$A^{[L]} = \\sigma(Z^{[L]})$. Y la derivada de su funcion de error con tespecto a A  es : $dA^{[L]}= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
        "\n",
        "donde $dA^{[L]}= \\frac YA + \\frac{(1-Y)}{(1-A)}$\n",
        "\n",
        "una forma de hacerlo con numpy es :\n",
        "\n",
        "```python\n",
        "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3bieU7pHtRkQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implementa el modulo de backward propagation para  [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
        "    \n",
        "    Argumentos:\n",
        "    AL -- vector de probabilidad, salida del forward propagation (L_model_forward())\n",
        "    Y -- clases \n",
        "    caches -- \n",
        "    \n",
        "    Returns:\n",
        "    grads -- Un diccionario de los gradientes\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "    grads={}\n",
        "    L=len(caches) # numero de capas\n",
        "    m =AL.shape[0]\n",
        "    eps = np.finfo(float).eps\n",
        "    \n",
        "    # Derivada de Loss con respecto a A \n",
        "    dAL =  - (np.divide(Y, AL + eps ) - np.divide(1 - Y, 1 - AL+ eps))\n",
        "    \n",
        "    # Inicializar Gradientes\n",
        "    # Backprop Sigmoide --> linear\n",
        "    grads['dA'+ str(L)], grads['dW'+str(L)], grads['db'+ str(L)] = linear_activation_backward(dAL,caches[L-1],activation='sigmoid')\n",
        "    \n",
        "    # Backprop Relu ---> Linear\n",
        "    for l in reversed(range(L-1)):\n",
        "       # current_cache = caches[l]\n",
        "        dA_prev_temp, dW_temp, db_temp = linear_activation_backward(grads['dA' + str(l+2)], caches[l], activation = \"relu\")\n",
        "        grads[\"dA\" + str(l + 1)] = dA_prev_temp\n",
        "        grads[\"dW\" + str(l + 1)] = dW_temp\n",
        "        grads[\"db\" + str(l + 1)] = db_temp\n",
        "    \n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SFwZVaFAtRkZ"
      },
      "cell_type": "markdown",
      "source": [
        "## 6  - Algoritmo de optimizacion \n",
        "\n",
        "Gradient descent: \n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "donde  $\\alpha$ es el  learning rate. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QOWS38xjtRka",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def update_parameters(parametros, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Gradient descent\n",
        "    \n",
        "    Argumentos:\n",
        "    parametros  \n",
        "    grads  \n",
        "    \n",
        "    Returns:\n",
        "    parametros -- actualizados \n",
        "    \"\"\"\n",
        "    # Numero de capas\n",
        "    L = len(parametros)//2 \n",
        "\n",
        "    for l in range(1,L):\n",
        "      parametros['W' + str(l)] += - learning_rate*grads['dW' + str(l)]\n",
        "      parametros['b' + str(l)] += - learning_rate*grads['db' + str(l)]\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YhTUYO1TMLUm",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Aplication"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "5AffIjcxCPy3",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def L_layer_model(X, Y, layers_dims, learning_rate=0.0075, num_iterations=3000, print_cost=False): \n",
        "\n",
        "    np.random.seed(1)\n",
        "    costs = []       \n",
        "    \n",
        "    # Parameters initialization.\n",
        "    parameters = initialize_parameters_deep(layers_dims)\n",
        "    \n",
        "    # Loop (gradient descent)\n",
        "    for i in range(0, num_iterations):\n",
        "\n",
        "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
        "        AL, caches = L_model_forward(X, parameters)\n",
        "        \n",
        "        # Compute cost.\n",
        "        cost = compute_cost(AL, Y)\n",
        "        \n",
        "        # Backward propagation.\n",
        "        grads = L_model_backward(AL, Y, caches)\n",
        "        \n",
        "        # Update parametros\n",
        "        parameters = update_parameters(parameters, grads, learning_rate)\n",
        "                \n",
        "        # Print the cost every 100 training example\n",
        "        if print_cost and i % 100 == 0:\n",
        "            print (\"Cost after iteration %i: %f\" % (i, cost))\n",
        "        if print_cost and i % 100 == 0:\n",
        "            costs.append(cost)\n",
        "            \n",
        "    # plot the cost\n",
        "    plt.plot(np.squeeze(costs))\n",
        "    plt.ylabel('cost')\n",
        "    plt.xlabel('iterations (per tens)')\n",
        "    plt.title(\"Learning rate =\" + str(learning_rate))\n",
        "    plt.show()\n",
        "    \n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LHn5X6nICQkM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "layers_dims = [784, 20, 7, 5, 10] #  5-layer model\n",
        "parametros= L_layer_model(X_train.T, Y.T, layers_dims,learning_rate=0.01, num_iterations=2500, print_cost=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "F_uVkHFCMdIN",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "EGGHKFbtCQsO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wyZx1h4otRkg"
      },
      "cell_type": "markdown",
      "source": [
        "## 7 Prediccion\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "H4X3feqp8yP9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(X, y, parametros):\n",
        "    \"\"\"\n",
        "    .\n",
        "    \n",
        "    Argumentos:\n",
        "    X -- data set \n",
        "    parametros\n",
        "    \n",
        "    Returns:\n",
        "    p -- predicciones del dataset dado X\n",
        "    \"\"\"\n",
        "    \n",
        "    \n",
        "        \n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mefIcTtE8xTw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1rP504iM8xa_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Wj6g0IMa9MtC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "uvCqKA3N9Mza",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}