{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "34EOh7oX6FoC"
   },
   "source": [
    "# Long Short Term Memory\n",
    "LSTMs are explicitly designed to avoid the long-term dependency problem. Remembering information for long periods of time is practically their default behavior, not something they struggle to learn.\n",
    "\n",
    "The key to LSTMs is the cell state, the horizontal line running through the top of the diagram.\n",
    "The cell state is kind of like a conveyor belt. It runs straight down the entire chain, with only some minor linear interactions. It’s very easy for information to just flow along it unchanged.\n",
    "\n",
    "![alt text](https://stanford.edu/~shervine/images/lstm.png)\n",
    "\n",
    "$$\\tilde{c}^{< t >}=\\textrm{tanh}(W_c[\\Gamma_r\\star a^{< t-1 >},x^{< t >}]+b_c)$$\n",
    "$$c^{< t >}= \\Gamma_u\\star\\tilde{c}^{< t >}+\\Gamma_f\\star c^{< t-1 >}$$\n",
    "$$a^{< t >}=\\Gamma_o\\star c^{< t >}$$\n",
    "\n",
    "The LSTM does have the ability to remove or add information to the cell state, carefully regulated by structures called gates.\n",
    "\n",
    "**Gates** :\n",
    " A system of gating units that controls the ﬂow of information\n",
    "* Update gate $\\Gamma_u$--> How much past should matter now?\n",
    "* Forget Gate $\\Gamma_f$-->Erase a cell or not?\n",
    "* Output gate $ \\Gamma_o$--> How much to reveal of a cell?\n",
    "* Reveleance gate  $ \\Gamma_r$-->  Drop previous information?\n",
    "\n",
    "[Understanding LSTMs](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "\n",
    "\n",
    "Here we are going to make Many to one LSTM network\n",
    "\n",
    "### Many to One\n",
    "\n",
    "$$T_x>1, T_y=1$$\n",
    "![alt text](https://stanford.edu/~shervine/images/rnn-many-to-one.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qm2LyZk07ai1"
   },
   "outputs": [],
   "source": [
    "! pip3 install torch torchvision\n",
    "# ! pip install torch_nightly -f https://download.pytorch.org/whl/nightly/cu92/torch_nightly.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tcYCWPI-6Z3i"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "zbDIu98s6Z-A"
   },
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device= torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BZ_wFgwJ6aDI"
   },
   "outputs": [],
   "source": [
    "#Hyperparameters\n",
    "sequence_length=28\n",
    "input_size=28\n",
    "hidden_size=128\n",
    "num_layers=2\n",
    "num_classes=10\n",
    "batch_size=100\n",
    "epochs=2\n",
    "learning_rate=0.01"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "u1NABxRSYa0V"
   },
   "source": [
    "### MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "o3DWoP-g6aH9",
    "outputId": "457e0687-e2e2-45c1-890f-8765b62554b7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "# MNIST dataset\n",
    "train_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                           train=True, \n",
    "                                           transform=transforms.ToTensor(),\n",
    "                                           download=True)\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(root='../../data/',\n",
    "                                          train=False, \n",
    "                                          transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xrmaG9l06aMv"
   },
   "outputs": [],
   "source": [
    "# Data loader\n",
    "train_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n",
    "                                           batch_size=batch_size, \n",
    "                                           shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(dataset=test_dataset,\n",
    "                                          batch_size=batch_size, \n",
    "                                          shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gSWHlmFt6aRx"
   },
   "source": [
    "## LSTM \n",
    "Many to One"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZUGodrfq6aXU"
   },
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(LSTM, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Set initial hidden and cell states \n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device) \n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "        \n",
    "        # Forward propagate LSTM\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: tensor of shape (batch_size, seq_length, hidden_size)\n",
    "        \n",
    "        # Decode the hidden state of the last time step\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "mX-MclO06adl"
   },
   "outputs": [],
   "source": [
    "\n",
    "model = LSTM(input_size, hidden_size, num_layers, num_classes).to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QgSftU0Jdlcz"
   },
   "source": [
    "### Loss and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QwE52FeKdymj"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ll30xUvVdzH3"
   },
   "source": [
    "### Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "GQB6ADu0d3bC",
    "outputId": "db81fa04-9ea6-4ea4-f4b8-1eb293acba4f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/2], Step [100/600], Loss: 0.2355\n",
      "Epoch [1/2], Step [200/600], Loss: 0.1477\n",
      "Epoch [1/2], Step [300/600], Loss: 0.2432\n",
      "Epoch [1/2], Step [400/600], Loss: 0.1224\n",
      "Epoch [1/2], Step [500/600], Loss: 0.0158\n",
      "Epoch [1/2], Step [600/600], Loss: 0.1991\n",
      "Epoch [2/2], Step [100/600], Loss: 0.2247\n",
      "Epoch [2/2], Step [200/600], Loss: 0.0908\n",
      "Epoch [2/2], Step [300/600], Loss: 0.0690\n",
      "Epoch [2/2], Step [400/600], Loss: 0.0720\n",
      "Epoch [2/2], Step [500/600], Loss: 0.1179\n",
      "Epoch [2/2], Step [600/600], Loss: 0.1952\n"
     ]
    }
   ],
   "source": [
    "total_step = len(train_loader)\n",
    "for epoch in range(epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i+1) % 100 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6tJiUdaWd3yG"
   },
   "source": [
    "### Test and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "-LRV7MCleD4o",
    "outputId": "3c77f2ed-6fb3-4667-b071-aeccec42cdce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 10000 test images: 97.68 %\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.reshape(-1, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print('Test Accuracy of the model on the 10000 test images: {} %'.format(100 * correct / total)) \n",
    "\n",
    "# Save the model checkpoint\n",
    "torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "27E6JY5FeHL0"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "LSTM with Pytorch.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
