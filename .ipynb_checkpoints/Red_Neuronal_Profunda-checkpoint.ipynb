{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Red_Neuronal_Profunda.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "gedLu2JxtRi4"
      },
      "cell_type": "markdown",
      "source": [
        "# Red Neuronal Profunda\n",
        "\n",
        "\n",
        "**Notacion**:\n",
        "- Superscript $[l]$ denota la cantidad  asociada con la capa $l^{th}$ . \n",
        "    - Ejemplo: $a^{[L]}$ es la $L^{th}$ capa de activacion. $W^{[L]}$ y $b^{[L]}$ son los parametros de la capa $L^{th}$ .\n",
        "- Superscript $(i)$ denota la cantidad asociada con el  $i^{th}$ ejemplo. \n",
        "    - Ejemplo: $x^{(i)}$ es el $i^{th}$ ejemplor de entrenamiento.\n",
        "- Lowerscript $i$ denota la $i^{th}$ entrada del vector.\n",
        "    - Ejemplo: $a^{[l]}_i$ denota ls $i^{th}$ entrada a la capa de acticacion $l^{th}$ \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "ObiQdt9rtRi6"
      },
      "cell_type": "markdown",
      "source": [
        "## 1 - Importar modulos\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "eySh3TyJtRi7",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "-CaJYMOGYzAi",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "DodBF-OwkRDJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "4AOixTRkkWhZ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "LT-B0DkRtRjE"
      },
      "cell_type": "markdown",
      "source": [
        "## 2 - Inicializacion\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Y7wfMa31tRjP"
      },
      "cell_type": "markdown",
      "source": [
        "### 2.1 - L-layer Neural Network\n",
        "\n",
        "construiremos una funcion para $L$ capas \n",
        " Recuerda que  $n^{[l]}$ es el  numero de unidades de la capa  $l$. Por ejepmplo si el tamaño de la entrada es  $X$ con forma $(12288, 209)$ (donde $m=209$ ejemplos) entonces:\n",
        "\n",
        "<table style=\"width:100%\">\n",
        "\n",
        "\n",
        "    <tr>\n",
        "        <td>  </td> \n",
        "        <td> **Shape of W** </td> \n",
        "        <td> **Shape of b**  </td> \n",
        "        <td> **Activation** </td>\n",
        "        <td> **Shape of Activation** </td> \n",
        "    <tr>\n",
        "    \n",
        "    <tr>\n",
        "        <td> **Layer 1** </td> \n",
        "        <td> $(n^{[1]},12288)$ </td> \n",
        "        <td> $(n^{[1]},1)$ </td> \n",
        "        <td> $Z^{[1]} = W^{[1]}  X + b^{[1]} $ </td> \n",
        "        \n",
        "        <td> $(n^{[1]},209)$ </td> \n",
        "    <tr>\n",
        "    \n",
        "    <tr>\n",
        "        <td> **Layer 2** </td> \n",
        "        <td> $(n^{[2]}, n^{[1]})$  </td> \n",
        "        <td> $(n^{[2]},1)$ </td> \n",
        "        <td>$Z^{[2]} = W^{[2]} A^{[1]} + b^{[2]}$ </td> \n",
        "        <td> $(n^{[2]}, 209)$ </td> \n",
        "    <tr>\n",
        "   \n",
        "       <tr>\n",
        "        <td> $\\vdots$ </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$  </td> \n",
        "        <td> $\\vdots$</td> \n",
        "        <td> $\\vdots$  </td> \n",
        "    <tr>\n",
        "    \n",
        "   <tr>\n",
        "        <td> **Layer L-1** </td> \n",
        "        <td> $(n^{[L-1]}, n^{[L-2]})$ </td> \n",
        "        <td> $(n^{[L-1]}, 1)$  </td> \n",
        "        <td>$Z^{[L-1]} =  W^{[L-1]} A^{[L-2]} + b^{[L-1]}$ </td> \n",
        "        <td> $(n^{[L-1]}, 209)$ </td> \n",
        "    <tr>\n",
        "    \n",
        "    \n",
        "   <tr>\n",
        "        <td> **Layer L** </td> \n",
        "        <td> $(n^{[L]}, n^{[L-1]})$ </td> \n",
        "        <td> $(n^{[L]}, 1)$ </td>\n",
        "        <td> $Z^{[L]} =  W^{[L]} A^{[L-1]} + b^{[L]}$</td>\n",
        "        <td> $(n^{[L]}, 209)$  </td> \n",
        "    <tr>\n",
        "\n",
        "</table>\n",
        "\n",
        "Recuerda que cuando computas  $W X + b$ en python, hace el broadcasting. Por ejemplo, si: \n",
        "\n",
        "$$ W = \\begin{bmatrix}\n",
        "    j  & k  & l\\\\\n",
        "    m  & n & o \\\\\n",
        "    p  & q & r \n",
        "\\end{bmatrix}\\;\\;\\; X = \\begin{bmatrix}\n",
        "    a  & b  & c\\\\\n",
        "    d  & e & f \\\\\n",
        "    g  & h & i \n",
        "\\end{bmatrix} \\;\\;\\; b =\\begin{bmatrix}\n",
        "    s  \\\\\n",
        "    t  \\\\\n",
        "    u\n",
        "\\end{bmatrix}\\tag{2}$$\n",
        "\n",
        "Entonces  $WX + b$ sera:\n",
        "\n",
        "$$ WX + b = \\begin{bmatrix}\n",
        "    (ja + kd + lg) + s  & (jb + ke + lh) + s  & (jc + kf + li)+ s\\\\\n",
        "    (ma + nd + og) + t & (mb + ne + oh) + t & (mc + nf + oi) + t\\\\\n",
        "    (pa + qd + rg) + u & (pb + qe + rh) + u & (pc + qf + ri)+ u\n",
        "\\end{bmatrix}\\tag{3}  $$"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "D3xzDgcptRjR"
      },
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Instruciones**:\n",
        "- La estructura del modelo es:   *[LINEAR -> RELU] $ \\times$ (L-1) -> LINEAR -> SIGMOID*. I.e., que tiene  $L-1$ capas usando a ReLU activation seguida de un sigmoide\n",
        "- Usa random initialization para las matrices weight. \n",
        "- Usa zeros initialization para los  biases\n",
        "- Nosotros guardaremos el valor  $n^{[l]}$, que es el numero de las unidades en las diferentes capas , en  la variable `layer_dims`. Por example, una `layer_dims` de forma [2,4,1]: Eso significa que tenemos dos entradas, una hidden layer con 4 hidden units, y una output layer una  unidad ."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "w2mwABhstRjR",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def initialize_parameters_deep(layer_dims):\n",
        "    \"\"\"\n",
        "    Argumentos:\n",
        "    layer_dims -- python array (list) que contiene las dimensiones de nuestra red neuronal \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- python diccionario que contiene los parametros \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
        "                    Wl -- weight matrix de la forma (layer_dims[l], layer_dims[l-1])\n",
        "                    bl -- bias vector de la forma (layer_dims[l], 1)\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "\n",
        "        \n",
        "    return parametros"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "ev5-jFlytRjU",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "XDAFYRL-tRjX"
      },
      "cell_type": "markdown",
      "source": [
        "## 3 - Forward propagation modulo\n",
        "\n",
        "### 3.1 - Linear Forward \n",
        "Modulo Linear Foward:\n",
        "\n",
        "- LINEAR\n",
        "- LINEAR -> ACTIVATION donde  ACTIVATION puede ser ReLU o Sigmoid. \n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID \n",
        "\n",
        "La funcion vectorizada para computar el Foward prop modulo sobre todos los ejemplos es:\n",
        "\n",
        "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{4}$$\n",
        "\n",
        "donde $A^{[0]} = X$. \n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "khkbmfnGtRjY",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_forward(A, W, b):\n",
        "    \"\"\"\n",
        "    Implementa el modulo de forward propagation para Lth capas.\n",
        "\n",
        "    Argumentos:\n",
        "    A -- activaciones de la capa anterior(o datos de entrada): (Tamaño de la capa anterior, numero de ejemplos)\n",
        "    W -- weights matrix: numpy array de la forma (Tamaño de la capa actual, Tamaño de la capa anterior)\n",
        "    b -- bias vector, numpy array de la forma (Tamaño de la capa actual, 1)\n",
        "\n",
        "    Returns:\n",
        "    Z -- la entrada de la activacion \n",
        "    cache --  \"A\", \"W\" y \"b\" \n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    \n",
        "    return Z, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "IE_IsZdtjvpV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9PeECgzDtRjf"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.2 - Linear-Activation Forward\n",
        "\n",
        "Funciones de activacion\n",
        "\n",
        "- **Sigmoid**: $\\sigma(Z) = \\sigma(W A + b) = \\frac{1}{ 1 + e^{-(W A + b)}}$.\n",
        "``` python\n",
        "A, activation_cache = sigmoid(Z)\n",
        "```\n",
        "\n",
        "- **ReLU**: La formula matematica  para ReLu es $A = RELU(Z) = max(0, Z)$. \n",
        "``` python\n",
        "A, activation_cache = relu(Z)\n",
        "```"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Tpqh7WghtRjg"
      },
      "cell_type": "markdown",
      "source": [
        "Implementar el paso de  forward propagation para la capa  *LINEAR->ACTIVATION* . \n",
        "\n",
        "La relacion matematica es la siguiente: $A^{[l]} = g(Z^{[l]}) = g(W^{[l]}A^{[l-1]} +b^{[l]})$ donde   \"g\"  es la fn activacion que puede ser ya sea sigmoid() o relu(). "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "qMoZo7VttRjg",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def linear_activation_forward(A_prev, W, b, activation):\n",
        "    \"\"\"\n",
        "    Implementar el paso de  forward propagation para la capa LINEAR->ACTIVATION \n",
        "\n",
        "    Argumentos:\n",
        "    A_prev -- activaciones de la capa anterior(o datos de entrada): (Tamaño de la capa anterior, numero de ejemplos)\n",
        "    W -- weights matrix: numpy array de la forma (Tamaño de la capa actual, Tamaño de la capa anterior)\n",
        "    b -- bias vector, numpy array de la forma (Tamaño de la capa actual, 1)\n",
        "    activation -- la fn de  activacion que sera usada en esta capa\n",
        "    \n",
        "    \n",
        "    Returns:\n",
        "    A -- post-activacion \n",
        "    cache --  diccionario \"linear_cache\" y \"activation_cache\";\n",
        "             \n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "\n",
        "    return A, cache"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "kT1OgS00tRjo"
      },
      "cell_type": "markdown",
      "source": [
        "### 3.3 Modelo de L capas\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "88eb9kQEtRjw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def L_model_forward(X, parametros):\n",
        "    \"\"\"\n",
        "    Implementa forward propagation para [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID \n",
        "    \n",
        "    Arguments:\n",
        "    X -- datos, numpy array de shape (Tamaño de entrada, numero de ejemplos)\n",
        "    parametros -- salida de  initialize_parameters_deep()\n",
        "    \n",
        "    Returns:\n",
        "    AL --  valor de post-activacion \n",
        "    caches \n",
        "    \"\"\"\n",
        "\n",
        "\n",
        "            \n",
        "    return AL, caches"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "mISbTkYMtRj2"
      },
      "cell_type": "markdown",
      "source": [
        "## 4 - Cost function\n",
        "\n",
        "\n",
        "**Crea la funcion**: Categorical cross-entropy cost \n",
        "$$J=-\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right)) \\tag{7}$$\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "9DuTcXaqtRj4",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def compute_cost(AL, Y):\n",
        "    \"\"\"\n",
        "    Implementa la cost function  (7).\n",
        "\n",
        "    Argumentoss:\n",
        "    AL -- vector de probabilidad correspondinte a las predicciones, shape (1, numero de ejemplos)\n",
        "    Y -- vector de  clases o categorias a predecir, shape (1, numero de ejemplos)\n",
        "\n",
        "    Returns:\n",
        "    cost -- cross-entropy cost\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    \n",
        "    return cost"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wLuT8r6LtRj9"
      },
      "cell_type": "markdown",
      "source": [
        "## 5 - Backward propagation modulo\n",
        "\n",
        "\n",
        "\n",
        "**Recordatorio sobre backprop** \n",
        "Recuerda que para encontrar los gradientes tienes que computar la derivada parcial $\\mathcal{L}$ con respecto a los parametros por ejemplo $z^{[1]}$ para una rnn de dos capas:\n",
        "\n",
        "$$\\frac{d \\mathcal{L}(a^{[2]},y)}{{dz^{[1]}}} = \\frac{d\\mathcal{L}(a^{[2]},y)}{{da^{[2]}}}\\frac{{da^{[2]}}}{{dz^{[2]}}}\\frac{{dz^{[2]}}}{{da^{[1]}}}\\frac{{da^{[1]}}}{{dz^{[1]}}} \\tag{8} $$\n",
        "\n",
        "\n",
        "\n",
        "Ahora computaremos el modulo de backprop en tres pasos:\n",
        "- LINEAR backward\n",
        "- LINEAR -> ACTIVATION backward donde ACTIVATION computa la derivada de  ReLU o sigmoid \n",
        "- [LINEAR -> RELU] $\\times$ (L-1) -> LINEAR -> SIGMOID backward "
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "pCbFW-EMtRj-"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.1 - Linear backward\n",
        "\n",
        "Formulas vectorizada para el modulo de bp\n",
        "\n",
        "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T} \\tag{8}$$\n",
        "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}\\tag{9}$$\n",
        "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]} \\tag{10}$$\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "e6kaKIaEtRkE",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_backward(dZ, cache):\n",
        "    \"\"\"\n",
        "    Implementa el modulo para  backward propagation para una capa\n",
        "\n",
        "    Argumentos:\n",
        "    dZ -- Gradiente del cost con respecto a la capa actual \n",
        "    cache -- tupla de los valores (A_prev, W, b) proveninetes del  forward propagation en la capa actual\n",
        "\n",
        "    Returns:\n",
        "    dA_prev -- Gradiente del cost con respecto a la activacion\n",
        "    dW -- Gradiente del  cost con respecto a W (capa actual l)\n",
        "    db -- Gradiente del  cost con respecto a b (capa actual l)\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RumwjG6ktRkI"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.2 - Linear-Activation backward\n",
        "\n",
        "\n",
        "\n",
        "Si $g(.)$ es la  fn activacion \n",
        "`sigmoid_backward` y `relu_backward` computan  la derivada de la fn actvivacion $$dZ^{[l]} = dA^{[l]} * g'(Z^{[l]}) \\tag{11}$$.  \n",
        "\n",
        "Implementa la capa de  backpropagation  *LINEAR->ACTIVATION* ."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "hgUSHulqtRkJ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def linear_activation_backward(dA, cache, activation):\n",
        "    \"\"\"\n",
        "    Implementa la capa de backward propagation para LINEAR->ACTIVATION\n",
        "    \n",
        "    Argumentos:\n",
        "    dA -- gradiente de post-activacion de l capa actual l \n",
        "    cache -- tuple de valores (linear_cache, activation_cache) \n",
        "    activation -- fn activacion\n",
        "    \n",
        "    Returns:\n",
        "    dA_prev -- Gradient del cost con respecto a la fn de activacion (de la capa anterior l-1)\n",
        "    dW -- Gradiente del  cost con respecto a W (capa actual l)\n",
        "    db -- Gradiente del  cost con respecto a b (capa actual l)\n",
        "    \"\"\"\n",
        "\n",
        " \n",
        "    \n",
        "    return dA_prev, dW, db"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "lSn-f6o9tRkP"
      },
      "cell_type": "markdown",
      "source": [
        "### 5.3 - L-Modelo Backward \n",
        "\n",
        "\n",
        "** Inicializacion de backpropagation**:\n",
        "Para hacer backprop atraves de la rednn, sabemos que la salida es, \n",
        "$A^{[L]} = \\sigma(Z^{[L]})$. Y la derivada de su funcion de error con tespecto a A  es : $dA^{[L]}= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$.\n",
        "\n",
        "donde $dA^{[L]}= \\frac YA + \\frac{(1-Y)}{(1-A)}$\n",
        "\n",
        "una forma de hacerlo con numpy es :\n",
        "\n",
        "```python\n",
        "dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL)) \n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "3bieU7pHtRkQ",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def L_model_backward(AL, Y, caches):\n",
        "    \"\"\"\n",
        "    Implementa el modulo de backward propagation para  [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID \n",
        "    \n",
        "    Argumentos:\n",
        "    AL -- vector de probabilidad, salida del forward propagation (L_model_forward())\n",
        "    Y -- clases \n",
        "    caches -- \n",
        "    \n",
        "    Returns:\n",
        "    grads -- Un diccionario de los gradientes\n",
        "             grads[\"dA\" + str(l)] = ... \n",
        "             grads[\"dW\" + str(l)] = ...\n",
        "             grads[\"db\" + str(l)] = ... \n",
        "    \"\"\"\n",
        "\n",
        "    \n",
        "      \n",
        "\n",
        "    return grads"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "SFwZVaFAtRkZ"
      },
      "cell_type": "markdown",
      "source": [
        "## 6  - Algoritmo de optimizacion \n",
        "\n",
        "Gradient descent: \n",
        "\n",
        "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]} \\tag{16}$$\n",
        "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]} \\tag{17}$$\n",
        "\n",
        "donde  $\\alpha$ es el  learning rate. "
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "QOWS38xjtRka",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def gradient_descent(parametros, grads, learning_rate):\n",
        "    \"\"\"\n",
        "    Gradient descent\n",
        "    \n",
        "    Argumentos:\n",
        "    parametros  \n",
        "    grads  \n",
        "    \n",
        "    Returns:\n",
        "    parameters -- actualizados \n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "    return parameters"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "wyZx1h4otRkg"
      },
      "cell_type": "markdown",
      "source": [
        "## 7 Prediccion\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "H4X3feqp8yP9",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def predict(X, y, parametros):\n",
        "    \"\"\"\n",
        "    .\n",
        "    \n",
        "    Argumentos:\n",
        "    X -- data set \n",
        "    parametros\n",
        "    \n",
        "    Returns:\n",
        "    p -- predicciones del dataset dado X\n",
        "    \"\"\"\n",
        "    \n",
        "\n",
        "        \n",
        "    return p"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "mefIcTtE8xTw",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "1rP504iM8xa_",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}