{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "XGBoost.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "M5TMFtth1Joj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Gradient Boosting Machines con XGBoost"
      ]
    },
    {
      "metadata": {
        "id": "8dOLm27cweft",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VMs_ZTmQwef_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "house_price=pd.read_csv('https://assets.datacamp.com/production/course_3679/datasets/ames_housing_trimmed_processed.csv')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "9M4GgC1jwegD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "f0704074-d97d-4e04-c895-ead6b80c3a91"
      },
      "cell_type": "code",
      "source": [
        "house_price.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>Remodeled</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>...</th>\n",
              "      <th>HouseStyle_1.5Unf</th>\n",
              "      <th>HouseStyle_1Story</th>\n",
              "      <th>HouseStyle_2.5Fin</th>\n",
              "      <th>HouseStyle_2.5Unf</th>\n",
              "      <th>HouseStyle_2Story</th>\n",
              "      <th>HouseStyle_SFoyer</th>\n",
              "      <th>HouseStyle_SLvl</th>\n",
              "      <th>PavedDrive_P</th>\n",
              "      <th>PavedDrive_Y</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>0</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>0</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>1</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>1</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>0</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 57 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   MSSubClass  LotFrontage  LotArea  OverallQual  OverallCond  YearBuilt  \\\n",
              "0          60         65.0     8450            7            5       2003   \n",
              "1          20         80.0     9600            6            8       1976   \n",
              "2          60         68.0    11250            7            5       2001   \n",
              "3          70         60.0     9550            7            5       1915   \n",
              "4          60         84.0    14260            8            5       2000   \n",
              "\n",
              "   Remodeled  GrLivArea  BsmtFullBath  BsmtHalfBath    ...      \\\n",
              "0          0       1710             1             0    ...       \n",
              "1          0       1262             0             1    ...       \n",
              "2          1       1786             1             0    ...       \n",
              "3          1       1717             1             0    ...       \n",
              "4          0       2198             1             0    ...       \n",
              "\n",
              "   HouseStyle_1.5Unf  HouseStyle_1Story  HouseStyle_2.5Fin  HouseStyle_2.5Unf  \\\n",
              "0                  0                  0                  0                  0   \n",
              "1                  0                  1                  0                  0   \n",
              "2                  0                  0                  0                  0   \n",
              "3                  0                  0                  0                  0   \n",
              "4                  0                  0                  0                  0   \n",
              "\n",
              "   HouseStyle_2Story  HouseStyle_SFoyer  HouseStyle_SLvl  PavedDrive_P  \\\n",
              "0                  1                  0                0             0   \n",
              "1                  0                  0                0             0   \n",
              "2                  1                  0                0             0   \n",
              "3                  1                  0                0             0   \n",
              "4                  1                  0                0             0   \n",
              "\n",
              "   PavedDrive_Y  SalePrice  \n",
              "0             1     208500  \n",
              "1             1     181500  \n",
              "2             1     223500  \n",
              "3             1     140000  \n",
              "4             1     250000  \n",
              "\n",
              "[5 rows x 57 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "metadata": {
        "id": "2Z190pKvwegI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Classification XGBoost\n",
        "### Breast Cancer Dataset"
      ]
    },
    {
      "metadata": {
        "id": "QAYjVAEawegJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "data = load_breast_cancer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "LQfUM9TnwegN",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "467ea3e3-1ef1-467e-dda4-6e03369055fa"
      },
      "cell_type": "code",
      "source": [
        "X_cancer=data.data\n",
        "y_cancer=data.target\n",
        "\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "\n",
        "# Create the training and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_cancer, y_cancer, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the classifier: dt_clf_4\n",
        "dt_clf_4 = DecisionTreeClassifier(max_depth=4)\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "dt_clf_4.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: y_pred_4\n",
        "y_pred_4 = dt_clf_4.predict(X_test)\n",
        "\n",
        "# Compute the accuracy of the predictions: accuracy\n",
        "accuracy = float(np.sum(y_pred_4==y_test))/y_test.shape[0]\n",
        "print(\"accuracy:\", accuracy)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.9649122807017544\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qiqW0xlKwegX",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "* XGBoost gets its lauded performance and efficiency gains by utilizing its own optimized data structure for datasets called a `DMatrix`\n"
      ]
    },
    {
      "metadata": {
        "id": "6Kzkj2RQwegY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "4b64f5dc-1ae2-4797-c3a0-9be4e897dca3"
      },
      "cell_type": "code",
      "source": [
        "# DMatrix\n",
        "cancer_Dmatrix= xgb.DMatrix(data=X_cancer, label=y_cancer)\n",
        "\n",
        "params={\"objective\":\"reg:logistic\",\"max_depth\":3}\n",
        "\n",
        "cv_results = xgb.cv(dtrain=cancer_Dmatrix, params=params, nfold=3, num_boost_round=5, metrics=\"error\", as_pandas=True, seed=123)\n",
        "\n",
        "print(cv_results)\n",
        "\n",
        "# Print the accuracy\n",
        "print(((1-cv_results[\"test-error-mean\"]).iloc[-1]))"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   test-error-mean  test-error-std  train-error-mean  train-error-std\n",
            "0         0.066824        0.019564          0.025480         0.002451\n",
            "1         0.061524        0.013876          0.021969         0.001257\n",
            "2         0.056252        0.010004          0.014945         0.006589\n",
            "3         0.052734        0.011418          0.012306         0.003300\n",
            "4         0.054497        0.012485          0.010549         0.004314\n",
            "0.9455026666666666\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "gA_y0qIKwegj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        },
        "outputId": "e89e587c-1c40-4d4f-b9ec-def7355f938b"
      },
      "cell_type": "code",
      "source": [
        "# Instantiate the XGBClassifier: xg_cl\n",
        "xg_cl = xgb.XGBClassifier(objective='binary:logistic', n_estimators=10, seed=123)\n",
        "\n",
        "# Fit the classifier to the training set\n",
        "xg_cl.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_cl.predict(X_test)\n",
        "\n",
        "# Compute the accuracy: accuracy\n",
        "accuracy = float(np.sum(preds==y_test))/y_test.shape[0]\n",
        "print(\"accuracy: %f\" % (accuracy))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "accuracy: 0.973684\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
            "  if diff:\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "ANzeAjaMwegu",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#### When to use XGBoost \n",
        "\n",
        "* You have a large number of training samples\n",
        "    * Greater than 1000 training samples and less 100 features\n",
        "    * The number of features < number of training samples\n",
        "* You have a mixture of categorical and numeric features\n",
        "    * Or just numeric features\n",
        "    \n",
        "#### When to NOT use XGBoost\n",
        "* Image recognition\n",
        "* Computer vision\n",
        "* Natural language processing and understanding problems\n",
        "* When the number of training samples is significantly smaller than the number of features\n",
        "\n",
        "## Regression with XGBoost\n",
        "\n",
        "#### Objective (loss) functions and base learners\n",
        "\n",
        "* Quantifies how far off a prediction is from the actual result\n",
        "* Measures the difference between estimated and true values for some collection of data\n",
        "* Goal: Find the model that yields the minimum value of the loss function\n",
        "Common Loss Functions and XGBoost\n",
        "\n",
        "#### Common Loss Functions and XGBoost\n",
        "* Loss function names in xgboost:\n",
        "    * reg:linear - use for regression problems\n",
        "    * reg:logistic - use for classification problems when you want just decision, not probability\n",
        "    * binary:logistic - use when you want probability rather than just decision\n",
        "    \n",
        "#### Base Learners and Why we need them\n",
        "* XGBoost involves creating a meta-model that is composed of many individual models that combine to give a final prediction\n",
        "* Individual models = base learners\n",
        "* Want base learners that when combined create final prediction that is non-linear\n",
        "* Each base learner should be good at distinguishing or predicting different parts of the dataset\n",
        "* Two kinds of base learners: tree and linear\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "651i7t7Dwegv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "44ad6de7-46ac-42e6-b550-7eda7d1c49f1"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "X_house=house_price.iloc[:,0:-1]\n",
        "y_house=house_price.iloc[:,-1]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_house, y_house, test_size=0.2, random_state=123)\n",
        "\n",
        "# Instantiate the XGBRegressor: xg_reg\n",
        "xg_reg = xgb.XGBRegressor(objective=\"reg:linear\",n_estimators=10,seed=123)\n",
        "\n",
        "# Fit the regressor to the training set\n",
        "xg_reg.fit(X_train,y_train)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_reg.predict(X_test)\n",
        "\n",
        "# Compute the rmse: rmse\n",
        "rmse = np.sqrt(mean_squared_error(preds, y_test))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 78847.401758\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5e1No3VGweg0",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Linear base learners\n",
        "Now that you've used trees as base models in XGBoost, let's use the other kind of base model that can be used with XGBoost - a linear learner. This model, although not as commonly used in XGBoost, allows you to create a regularized linear regression using XGBoost's powerful learning API. However, because it's uncommon, you have to use XGBoost's own non-scikit-learn compatible functions to build the model, such as `xgb.train()`\n",
        "In order to do this you must create the parameter dictionary that describes the kind of booster you want to use\n",
        "The key-value pair that defines the booster type (base model) you need is \"booster\":\"gblinear\""
      ]
    },
    {
      "metadata": {
        "id": "RkQgu1Aaweg1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "bf202109-6c70-43b6-93eb-c6bfa7329d81"
      },
      "cell_type": "code",
      "source": [
        "# Convert the training and testing sets into DMatrixes: DM_train, DM_test\n",
        "DM_train = xgb.DMatrix(data=X_train,label=y_train)\n",
        "DM_test =  xgb.DMatrix(data=X_test,label=y_test)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"booster\":\"gblinear\", \"objective\":\"reg:linear\"}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train( dtrain= DM_train, params=params, num_boost_round=5)\n",
        "\n",
        "# Predict the labels of the test set: preds\n",
        "preds = xg_reg.predict(DM_test)\n",
        "\n",
        "rmse = np.sqrt(mean_squared_error(y_test,preds))\n",
        "print(\"RMSE: %f\" % (rmse))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "RMSE: 43566.535658\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "vtQzZKSzweg7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        },
        "outputId": "734779b6-3832-42da-f21d-02b6b1f5603c"
      },
      "cell_type": "code",
      "source": [
        "housing_dmatrix = xgb.DMatrix(data=X_house, label=y_house)\n",
        "\n",
        "# Create the parameter dictionary: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=4, num_boost_round=5, metrics='mae', as_pandas=True, seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)\n",
        "\n",
        "# Extract and print final boosting round metric\n",
        "print((cv_results[\"test-mae-mean\"]).tail(1))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   test-mae-mean  test-mae-std  train-mae-mean  train-mae-std\n",
            "0  127634.000000   2404.009898   127343.482421     668.308109\n",
            "1   90122.501953   2107.912810    89770.056641     456.965267\n",
            "2   64278.558594   1887.567576    63580.791016     263.404950\n",
            "3   46819.168945   1459.818607    45633.155274     151.883420\n",
            "4   35670.646484   1140.607452    33587.090820      86.999396\n",
            "4    35670.646484\n",
            "Name: test-mae-mean, dtype: float64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "bxli_FtLwehA",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Regularization and base learners in XGBoost\n",
        "* Regularization is a control on model complexity\n",
        "* Want models that are both accurate and as simple as possible\n",
        "* Regularization parameters in XGBoost:\n",
        "    * gamma - minimum loss reduction allowed for a split to occur\n",
        "    * alpha - l1 regularization on leaf weights, larger values mean more regularization\n",
        "    * lambda - l2 regularization on leaf weights\n",
        "    \n",
        "#### Base Learners in XGBoost\n",
        "* Linear Base Learner:\n",
        "Sum of linear terms\n",
        "Boosted model is weighted sum of linear models (thus is itself linear)\n",
        "Rarely used\n",
        "* Tree Base Learner:\n",
        "Decision tree\n",
        "Boosted model is weighted sum of decision trees (nonlinear)\n",
        "Almost exclusively used in XGBoost\n",
        "\n",
        "#### Creating DataFrames from multiple equal-length lists\n",
        "* `pd.DataFrame(list(zip(list1,list2)),columns=[\"list1\",\"list2\"]))\n",
        "* zip creates a generator of parallel values:\n",
        "    * zip([1,2,3],[\"a\",\"b\"\"c\"]) = [1,\"a\"],[2,\"b\"],[3,\"c\"]\n",
        "    * generators need to be completely instantiated before they can be used in DataFrame objects\n",
        "* list() instantiates the full generator and passing that into the DataFrame converts the whole expression`"
      ]
    },
    {
      "metadata": {
        "id": "kcay2VrowehB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "3b78a787-f783-4423-9fec-15c8f5dd23d8"
      },
      "cell_type": "code",
      "source": [
        "reg_params = [1, 10, 100]\n",
        "\n",
        "# Create the initial parameter dictionary for varying l2 strength: params\n",
        "params = {\"objective\":\"reg:linear\",\"max_depth\":3}\n",
        "\n",
        "# Create an empty list for storing rmses as a function of l2 complexity\n",
        "rmses_l2 = []\n",
        "\n",
        "# Iterate over reg_params\n",
        "for reg in reg_params:\n",
        "\n",
        "    # Update l2 strength\n",
        "    params[\"lambda\"] = reg\n",
        "    \n",
        "    # Pass this updated param dictionary into cv\n",
        "    cv_results_rmse = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2, num_boost_round=5, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append best rmse (final round) to rmses_l2\n",
        "    rmses_l2.append(cv_results_rmse[\"test-rmse-mean\"].tail(1).values[0])\n",
        "\n",
        "# Look at best rmse per l2 param\n",
        "print(\"Best rmse as a function of l2:\")\n",
        "print(pd.DataFrame(list(zip(reg_params, rmses_l2)), columns=[\"l2\", \"rmse\"]))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best rmse as a function of l2:\n",
            "    l2          rmse\n",
            "0    1  52275.357421\n",
            "1   10  57746.064453\n",
            "2  100  76624.625000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "qNYhXzC1wehF",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing individual XGBoost trees\n",
        "Now that you've used XGBoost to both build and evaluate regression as well as classification models, you should get a handle on how to visually explore your models. Here, you will visualize individual trees from the fully boosted model that XGBoost creates using the entire housing dataset.\n",
        "\n",
        "XGBoost has a `plot_tree()` function that makes this type of visualization easy. Once you train a model using the XGBoost learning API, you can pass it to the `plot_tree()` function along with the number of trees you want to plot using the num_trees argument."
      ]
    },
    {
      "metadata": {
        "id": "JKvkOqf_wehG",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "params = {\"objective\":\"reg:linear\", \"max_depth\":2}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the first tree\n",
        "xgb.plot_tree(xg_reg,num_trees=0)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "ZWmKmSqdwehK",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot the fifth tree\n",
        "xgb.plot_tree(xg_reg,num_trees=4)\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(50, 25)\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sdwaMrGWwehO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Plot the last tree sideways\n",
        "xgb.plot_tree(xg_reg,num_trees=9,rankdir=\"LR\")\n",
        "fig = plt.gcf()\n",
        "fig.set_size_inches(100, 50)\n",
        "fig.savefig('tree.png')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qRQnL4AbwehS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Visualizing feature importances: What features are most important in my dataset\n",
        "Another way to visualize your XGBoost models is to examine the importance of each feature column in the original dataset within the model.\n",
        "\n",
        "One simple way of doing this involves counting the number of times each feature is split on across all boosting rounds (trees) in the model, and then visualizing the result as a bar graph, with the features ordered according to how many times they appear. XGBoost has a `plot_importance()` function that allows you to do exactly this"
      ]
    },
    {
      "metadata": {
        "id": "GTIb7kYGwehT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 376
        },
        "outputId": "cd9d5174-adf0-4519-edbf-60eb4a8dd12a"
      },
      "cell_type": "code",
      "source": [
        "params = {\"objective\":\"reg:linear\",\"max_depth\":4}\n",
        "\n",
        "# Train the model: xg_reg\n",
        "xg_reg = xgb.train(params=params, dtrain=housing_dmatrix, num_boost_round=10)\n",
        "\n",
        "# Plot the feature importances\n",
        "xgb.plot_importance(xg_reg)\n",
        "plt.show()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAFnCAYAAAC7JX2VAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMIHQBAAAYAKhCQAAwIR/AGbGDm0EdLfBAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7ff6c73b4fd0>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "fzNmDe_jwehW",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tuning the number of boosting rounds\n",
        "Let's start with parameter tuning by seeing how the number of boosting rounds (number of trees you build) impacts the out-of-sample performance of your XGBoost model. You'll use xgb.cv() inside a for loop and build one model per num_boost_round parameter."
      ]
    },
    {
      "metadata": {
        "id": "qul2OBtTwehY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "c64b7f35-69f9-4419-ac5e-d3aa57df95b5"
      },
      "cell_type": "code",
      "source": [
        "# Create the parameter dictionary for each tree: params \n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
        "\n",
        "# Create list of number of boosting rounds\n",
        "num_rounds = [5, 10, 15]\n",
        "\n",
        "# Empty list to store final round rmse per XGBoost model\n",
        "final_rmse_per_round = []\n",
        "\n",
        "# Iterate over num_rounds and build one model per num_boost_round parameter\n",
        "for curr_num_rounds in num_rounds:\n",
        "\n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=3, num_boost_round=curr_num_rounds, metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append final round RMSE\n",
        "    final_rmse_per_round.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "num_rounds_rmses = list(zip(num_rounds, final_rmse_per_round))\n",
        "print(pd.DataFrame(num_rounds_rmses,columns=[\"num_boosting_rounds\",\"rmse\"]))"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   num_boosting_rounds          rmse\n",
            "0                    5  50903.299479\n",
            "1                   10  34774.194010\n",
            "2                   15  32895.098958\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "0RSHauV4wehd",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Automated boosting round selection using early_stopping\n",
        "Now, instead of attempting to cherry pick the best possible number of boosting rounds, you can very easily have XGBoost automatically select the number of boosting rounds for you within `xgb.cv()`. This is done using a technique called early stopping.\n",
        "\n",
        "Early stopping works by testing the XGBoost model after every boosting round against a hold-out dataset and stopping the creation of additional boosting rounds (thereby finishing training of the model early) if the hold-out metric (\"rmse\" in our case) does not improve for a given number of rounds. Here you will use the `early_stopping_rounds` parameter in `xgb.cv()` with a large possible number of boosting rounds (50). Bear in mind that if the holdout metric continuously improves up through when `num_boosting_rounds` is reached, then early stopping does not occur."
      ]
    },
    {
      "metadata": {
        "id": "GqzA0srYwehe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 884
        },
        "outputId": "3a8e4a26-51ac-4226-a495-37edcb258e6c"
      },
      "cell_type": "code",
      "source": [
        "# Create the parameter dictionary for each tree: params\n",
        "params = {\"objective\":\"reg:linear\", \"max_depth\":4}\n",
        "\n",
        "# Perform cross-validation with early stopping: cv_results\n",
        "cv_results = xgb.cv(dtrain=housing_dmatrix,params=params,nfold=3,num_boost_round=50,metrics='rmse', early_stopping_rounds=10,as_pandas=True,seed=123)\n",
        "\n",
        "# Print cv_results\n",
        "print(cv_results)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    test-rmse-mean  test-rmse-std  train-rmse-mean  train-rmse-std\n",
            "0    142640.656250     705.559400    141871.630208      403.632626\n",
            "1    104907.664063     111.113862    103057.036458       73.769561\n",
            "2     79262.059895     563.766991     75975.966146      253.726099\n",
            "3     61620.136719    1087.694282     57420.529948      521.658354\n",
            "4     50437.562500    1846.448017     44552.955729      544.170190\n",
            "5     43035.658854    2034.471024     35763.949219      681.798925\n",
            "6     38600.880208    2169.796232     29861.464844      769.571318\n",
            "7     36071.817708    2109.795430     25994.675781      756.521419\n",
            "8     34383.184896    1934.546688     23306.836588      759.238254\n",
            "9     33509.139974    1887.375633     21459.770833      745.624404\n",
            "10    32916.805990    1850.893363     20148.721354      749.612769\n",
            "11    32197.832682    1734.456935     19215.382813      641.387376\n",
            "12    31770.852865    1802.155484     18627.389323      716.256596\n",
            "13    31482.782552    1779.123767     17960.695312      557.043568\n",
            "14    31389.990234    1892.319927     17559.736979      631.412969\n",
            "15    31302.883464    1955.166046     17205.712891      590.171393\n",
            "16    31234.058594    1880.705796     16876.571940      703.631755\n",
            "17    31318.347656    1828.860164     16597.662110      703.677609\n",
            "18    31323.634766    1775.909567     16330.460937      607.274494\n",
            "19    31204.135417    1739.076156     16005.972982      520.470911\n",
            "20    31089.863932    1756.022575     15814.300781      518.604760\n",
            "21    31047.998047    1624.672407     15493.405924      505.616658\n",
            "22    31056.916667    1668.043013     15270.734375      502.018453\n",
            "23    31024.983724    1548.985354     15086.382162      503.913199\n",
            "24    30983.685547    1663.130510     14917.608399      486.206187\n",
            "25    30989.477214    1686.668050     14709.589518      449.668010\n",
            "26    30952.113932    1613.172643     14457.286133      376.787666\n",
            "27    31066.902344    1648.534310     14185.567057      383.102691\n",
            "28    31095.642578    1709.225327     13934.066732      473.465449\n",
            "29    31103.887370    1778.880069     13749.644857      473.670886\n",
            "30    30976.084635    1744.514164     13549.836589      454.898834\n",
            "31    30938.469401    1746.052597     13413.484700      399.603323\n",
            "32    30931.000000    1772.469510     13275.915364      415.408340\n",
            "33    30929.056641    1765.541578     13085.878255      493.792778\n",
            "34    30890.629557    1786.510976     12947.181315      517.790039\n",
            "35    30884.493490    1769.729143     12846.027344      547.732372\n",
            "36    30833.542318    1691.001567     12702.378581      505.523315\n",
            "37    30856.688151    1771.445059     12532.243815      508.298162\n",
            "38    30818.016927    1782.784630     12384.055013      536.225042\n",
            "39    30839.392578    1847.327022     12198.443359      545.165562\n",
            "40    30776.964844    1912.781000     12054.583659      508.841772\n",
            "41    30794.702474    1919.674832     11897.036458      477.177568\n",
            "42    30780.956380    1906.820987     11756.221354      502.992395\n",
            "43    30783.754557    1951.259784     11618.846680      519.837469\n",
            "44    30776.731120    1953.447693     11484.080404      578.428621\n",
            "45    30758.543620    1947.454953     11356.552734      565.368794\n",
            "46    30729.971354    1985.698867     11193.557943      552.299272\n",
            "47    30732.662760    1966.997355     11071.315755      604.090310\n",
            "48    30712.241536    1957.751573     10950.778320      574.862779\n",
            "49    30720.854167    1950.511057     10824.865560      576.665674\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "uNhYbVBkwehk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Tunable parameters in XGBoost\n",
        "### Common tree tunable parameters\n",
        "* learning rate: learning rate/eta\n",
        "* gamma: min loss reduction to create new tree split\n",
        "* lambda: L2 reg on leaf weights\n",
        "* alpha: L1 reg on leaf weights\n",
        "* max_depth: max depth per tree\n",
        "* subsample: % samples used per tree\n",
        "* colsample_bytree: % features used per tree\n",
        "\n",
        "### Linear tunable parameters\n",
        "* lambda: L2 reg on weights\n",
        "* alpha: L1 reg on weights\n",
        "* lambda_bias: L2 reg term on bias\n",
        "* You can also tune the number of estimators used for both base model types!\n",
        "\n",
        "#### Tuning eta\n",
        "It's time to practice tuning other XGBoost hyperparameters in earnest and observing their effect on model performance! You'll begin by tuning the \"eta\", also known as the learning rate.\n",
        "\n",
        "The learning rate in XGBoost is a parameter that can range between 0 and 1, with higher values of \"eta\" penalizing feature weights more strongly, causing much stronger regularization."
      ]
    },
    {
      "metadata": {
        "id": "SYgwPNYpwehk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "7bb77b50-6857-4632-e960-176fd81fb4d4"
      },
      "cell_type": "code",
      "source": [
        "params = {\"objective\":\"reg:linear\", \"max_depth\":3}\n",
        "\n",
        "# Create list of eta values and empty list to store final round rmse per xgboost model\n",
        "eta_vals = [0.001, 0.01, 0.1]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the eta \n",
        "for curr_val in eta_vals:\n",
        "\n",
        "    params[\"rmse\"] = curr_val\n",
        "    \n",
        "    # Perform cross-validation: cv_results\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params,metrics='rmse',num_boost_round=10,early_stopping_rounds=5,nfold=3,seed=123,as_pandas=True)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(eta_vals, best_rmse)), columns=[\"eta\",\"best_rmse\"]))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "     eta    best_rmse\n",
            "0  0.001  34774.19401\n",
            "1  0.010  34774.19401\n",
            "2  0.100  34774.19401\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "T-g_sYQBweho",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tuning max_depth\n",
        "In this exercise, your job is to tune `max_depth`, which is the parameter that dictates the maximum depth that each tree in a boosting round can grow to. Smaller values will lead to shallower trees, and larger values to deeper trees."
      ]
    },
    {
      "metadata": {
        "id": "zGHntRnLwehp",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "2c3d7055-2e57-419a-ba98-f5a975e5495a"
      },
      "cell_type": "code",
      "source": [
        "params = {\"objective\":\"reg:linear\"}\n",
        "\n",
        "# Create list of max_depth values\n",
        "max_depths = [2,5,10,20]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the max_depth\n",
        "for curr_val in max_depths:\n",
        "\n",
        "    params[\"max_depths\"] = curr_val\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix,params=params,metrics='rmse',nfold=2,num_boost_round=10,early_stopping_rounds=5,seed=123,as_pandas=True)\n",
        "    \n",
        "    \n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(max_depths, best_rmse)),columns=[\"max_depth\",\"best_rmse\"]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   max_depth     best_rmse\n",
            "0          2  35922.521484\n",
            "1          5  35922.521484\n",
            "2         10  35922.521484\n",
            "3         20  35922.521484\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Xkbq-YD3wehs",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Tuning colsample_bytree\n",
        "Now, it's time to tune \"colsample_bytree\". You've already seen this if you've ever worked with scikit-learn's RandomForestClassifier or RandomForestRegressor, where it just was called max_features. In both xgboost and sklearn, this parameter (although named differently) simply specifies the fraction of features to choose from at every split in a given tree. In xgboost, colsample_bytree must be specified as a float between 0 and 1."
      ]
    },
    {
      "metadata": {
        "id": "CKaxoz4zwehs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 102
        },
        "outputId": "33ac3891-bbc1-4883-cc2c-11a6a70df39e"
      },
      "cell_type": "code",
      "source": [
        "params={\"objective\":\"reg:linear\",\"max_depth\":3}\n",
        "\n",
        "# Create list of hyperparameter values: colsample_bytree_vals\n",
        "colsample_bytree_vals = [0.1,0.5,0.8,1]\n",
        "best_rmse = []\n",
        "\n",
        "# Systematically vary the hyperparameter value \n",
        "for curr_val in colsample_bytree_vals:\n",
        "\n",
        "    params['colsample_bytree'] = curr_val\n",
        "    \n",
        "    # Perform cross-validation\n",
        "    cv_results = xgb.cv(dtrain=housing_dmatrix, params=params, nfold=2,\n",
        "                 num_boost_round=10, early_stopping_rounds=5,\n",
        "                 metrics=\"rmse\", as_pandas=True, seed=123)\n",
        "    \n",
        "    # Append the final round rmse to best_rmse\n",
        "    best_rmse.append(cv_results[\"test-rmse-mean\"].tail().values[-1])\n",
        "\n",
        "# Print the resultant DataFrame\n",
        "print(pd.DataFrame(list(zip(colsample_bytree_vals, best_rmse)), columns=[\"colsample_bytree\",\"best_rmse\"]))"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   colsample_bytree     best_rmse\n",
            "0               0.1  44363.458985\n",
            "1               0.5  36266.462890\n",
            "2               0.8  35704.357422\n",
            "3               1.0  35836.046875\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "YeuV9DSTwehw",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Grid Search and Random Search\n",
        "#### Grid Search: \n",
        "* Search exhaustively over a given set of hyperparameters, once per set of hyperparameters\n",
        "* Number of models = number of distinct values per hyperparameter multiplied across each hyperparameter\n",
        "* Pick final model hyperparameter values that give best cross-validated evaluation metric value\n",
        "\n",
        "#### Random Search: \n",
        "* Create a (possibly infinite) range of hyperparameter values per hyperparameter that you would like to search over\n",
        "* Set the number of iterations you would like for the random search to continue\n",
        "* During each iteration, randomly draw a value in the range of specified values for each hyperparameter searched over and train/evaluate a model with those hyperparameters\n",
        "* After you've reached the maximum number of iterations, select the hyperparameter configuration with the best evaluated score\n",
        "\n",
        "#### Grid Search with XGBoost\n",
        "Now that you've learned how to tune parameters individually with XGBoost, let's take your parameter tuning to the next level by using scikit-learn's GridSearch and RandomizedSearch capabilities with internal cross-validation using the GridSearchCV and RandomizedSearchCV functions. You will use these to find the best model exhaustively from a collection of possible parameter values across multiple parameters simultaneously. Let's get to work, starting with GridSearchCV"
      ]
    },
    {
      "metadata": {
        "id": "dKQHnorlwehw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "a84a63f5-ce05-4aa4-cbe9-b568ae969e49"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "gbm_param_grid = {\n",
        "    'colsample_bytree': [0.3, 0.7],\n",
        "    'n_estimators': [50],\n",
        "    'max_depth': [2, 5]\n",
        "}\n",
        "\n",
        "# Instantiate the regressor: gbm\n",
        "gbm = xgb.XGBRegressor()\n",
        "\n",
        "# Perform grid search: grid_mse\n",
        "grid_mse = GridSearchCV(estimator=gbm,param_grid=gbm_param_grid,scoring='neg_mean_squared_error',cv=4,verbose=1)\n",
        "\n",
        "\n",
        "# Fit grid_mse to the data\n",
        "grid_mse.fit(X_house,y_house)\n",
        "\n",
        "# Print the best parameters and lowest RMSE\n",
        "print(\"Best parameters found: \", grid_mse.best_params_)\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(grid_mse.best_score_)))\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 4 folds for each of 4 candidates, totalling 16 fits\n",
            "Best parameters found:  {'colsample_bytree': 0.3, 'max_depth': 5, 'n_estimators': 50}\n",
            "Lowest RMSE found:  29655.33697347771\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=1)]: Done  16 out of  16 | elapsed:    1.4s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "id": "isB9zZ12wehz",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Random Search with XGBoost\n",
        "Often, GridSearchCV can be really time consuming, so in practice, you may want to use RandomizedSearchCV instead, as you will do in this exercise. The good news is you only have to make a few modifications to your GridSearchCV code to do RandomizedSearchCV. The key difference is you have to specify a param_distributions parameter instead of a param_grid parameter"
      ]
    },
    {
      "metadata": {
        "id": "K3-IJpgTweh0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "6855a8de-6fc0-4770-abb7-c1e7515d5f63"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "\n",
        "gbm_param_grid = {\n",
        "    'n_estimators': [25],\n",
        "    'max_depth': range(2, 12)\n",
        "}\n",
        "\n",
        "# Instantiate the regressor: gbm\n",
        "gbm = xgb.XGBRegressor(n_estimators=10)\n",
        "\n",
        "# Perform random search: grid_mse\n",
        "randomized_mse = RandomizedSearchCV(estimator=gbm,param_distributions=gbm_param_grid,scoring='neg_mean_squared_error',n_iter=5,cv=4,)\n",
        "\n",
        "\n",
        "# Fit randomized_mse to the data\n",
        "randomized_mse.fit(X_house,y_house)\n",
        "\n",
        "# Print the best parameters and lowest RMSE\n",
        "print(\"Best parameters found: \", randomized_mse.best_params_)\n",
        "print(\"Lowest RMSE found: \", np.sqrt(np.abs(randomized_mse.best_score_)))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameters found:  {'n_estimators': 25, 'max_depth': 5}\n",
            "Lowest RMSE found:  36636.35808132903\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6OrNCJqrweh3",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Grid Search and Random Search Limitations\n",
        "* Grid Search\n",
        "    * Number of models you must build with every additional new parameter grows very quickly\n",
        "* Random Search\n",
        "    * Parameter space to explore can be massive\n",
        "    * Randomly jumping throughout the space looking for a \"best\" result becomes a waiting game\n",
        "    \n",
        "## Using XGBoost in pipelines\n",
        "### Preprocessing I: LabelEncoder and OneHotEncoder\n",
        "* **LabelEncoder**: Converts a categorical column of strings into integers\n",
        "* **OneHotEncoder**: Takes the column of integers and encodes them as dummy variables\n",
        "* Cannot be done in a pipeline\n",
        "\n",
        "### Preprocessing II: DictVectorizer\n",
        "* Traditionally used in text processing\n",
        "* Converts lists of feature mappings into vectors\n",
        "* Need to convert DataFrame into a list of dictionary entries"
      ]
    },
    {
      "metadata": {
        "id": "BTh2RNAzweh4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import cross_val_score"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "VCNMri-5weh-",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "unproc_house=pd.read_csv('https://assets.datacamp.com/production/course_3679/datasets/ames_unprocessed_data.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "pCr8ySDwweiD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 253
        },
        "outputId": "0233550d-c1ab-46b1-d28a-0b1a9fd7a866"
      },
      "cell_type": "code",
      "source": [
        "unproc_house.head()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>MSSubClass</th>\n",
              "      <th>MSZoning</th>\n",
              "      <th>LotFrontage</th>\n",
              "      <th>LotArea</th>\n",
              "      <th>Neighborhood</th>\n",
              "      <th>BldgType</th>\n",
              "      <th>HouseStyle</th>\n",
              "      <th>OverallQual</th>\n",
              "      <th>OverallCond</th>\n",
              "      <th>YearBuilt</th>\n",
              "      <th>...</th>\n",
              "      <th>GrLivArea</th>\n",
              "      <th>BsmtFullBath</th>\n",
              "      <th>BsmtHalfBath</th>\n",
              "      <th>FullBath</th>\n",
              "      <th>HalfBath</th>\n",
              "      <th>BedroomAbvGr</th>\n",
              "      <th>Fireplaces</th>\n",
              "      <th>GarageArea</th>\n",
              "      <th>PavedDrive</th>\n",
              "      <th>SalePrice</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>65.0</td>\n",
              "      <td>8450</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2003</td>\n",
              "      <td>...</td>\n",
              "      <td>1710</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "      <td>548</td>\n",
              "      <td>Y</td>\n",
              "      <td>208500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>20</td>\n",
              "      <td>RL</td>\n",
              "      <td>80.0</td>\n",
              "      <td>9600</td>\n",
              "      <td>Veenker</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>1Story</td>\n",
              "      <td>6</td>\n",
              "      <td>8</td>\n",
              "      <td>1976</td>\n",
              "      <td>...</td>\n",
              "      <td>1262</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>460</td>\n",
              "      <td>Y</td>\n",
              "      <td>181500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>68.0</td>\n",
              "      <td>11250</td>\n",
              "      <td>CollgCr</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>2001</td>\n",
              "      <td>...</td>\n",
              "      <td>1786</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>608</td>\n",
              "      <td>Y</td>\n",
              "      <td>223500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>70</td>\n",
              "      <td>RL</td>\n",
              "      <td>60.0</td>\n",
              "      <td>9550</td>\n",
              "      <td>Crawfor</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>7</td>\n",
              "      <td>5</td>\n",
              "      <td>1915</td>\n",
              "      <td>...</td>\n",
              "      <td>1717</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>642</td>\n",
              "      <td>Y</td>\n",
              "      <td>140000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>60</td>\n",
              "      <td>RL</td>\n",
              "      <td>84.0</td>\n",
              "      <td>14260</td>\n",
              "      <td>NoRidge</td>\n",
              "      <td>1Fam</td>\n",
              "      <td>2Story</td>\n",
              "      <td>8</td>\n",
              "      <td>5</td>\n",
              "      <td>2000</td>\n",
              "      <td>...</td>\n",
              "      <td>2198</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>1</td>\n",
              "      <td>836</td>\n",
              "      <td>Y</td>\n",
              "      <td>250000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 21 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   MSSubClass MSZoning  LotFrontage  LotArea Neighborhood BldgType HouseStyle  \\\n",
              "0          60       RL         65.0     8450      CollgCr     1Fam     2Story   \n",
              "1          20       RL         80.0     9600      Veenker     1Fam     1Story   \n",
              "2          60       RL         68.0    11250      CollgCr     1Fam     2Story   \n",
              "3          70       RL         60.0     9550      Crawfor     1Fam     2Story   \n",
              "4          60       RL         84.0    14260      NoRidge     1Fam     2Story   \n",
              "\n",
              "   OverallQual  OverallCond  YearBuilt    ...     GrLivArea  BsmtFullBath  \\\n",
              "0            7            5       2003    ...          1710             1   \n",
              "1            6            8       1976    ...          1262             0   \n",
              "2            7            5       2001    ...          1786             1   \n",
              "3            7            5       1915    ...          1717             1   \n",
              "4            8            5       2000    ...          2198             1   \n",
              "\n",
              "   BsmtHalfBath  FullBath  HalfBath  BedroomAbvGr  Fireplaces  GarageArea  \\\n",
              "0             0         2         1             3           0         548   \n",
              "1             1         2         0             3           1         460   \n",
              "2             0         2         1             3           1         608   \n",
              "3             0         1         0             3           1         642   \n",
              "4             0         2         1             4           1         836   \n",
              "\n",
              "   PavedDrive SalePrice  \n",
              "0           Y    208500  \n",
              "1           Y    181500  \n",
              "2           Y    223500  \n",
              "3           Y    140000  \n",
              "4           Y    250000  \n",
              "\n",
              "[5 rows x 21 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "metadata": {
        "id": "4Q0MFVQ8weiH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "outputId": "df433497-bce1-4b76-f377-f1d54593bb0b"
      },
      "cell_type": "code",
      "source": [
        "unproc_house.info()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1460 entries, 0 to 1459\n",
            "Data columns (total 21 columns):\n",
            "MSSubClass      1460 non-null int64\n",
            "MSZoning        1460 non-null object\n",
            "LotFrontage     1201 non-null float64\n",
            "LotArea         1460 non-null int64\n",
            "Neighborhood    1460 non-null object\n",
            "BldgType        1460 non-null object\n",
            "HouseStyle      1460 non-null object\n",
            "OverallQual     1460 non-null int64\n",
            "OverallCond     1460 non-null int64\n",
            "YearBuilt       1460 non-null int64\n",
            "Remodeled       1460 non-null int64\n",
            "GrLivArea       1460 non-null int64\n",
            "BsmtFullBath    1460 non-null int64\n",
            "BsmtHalfBath    1460 non-null int64\n",
            "FullBath        1460 non-null int64\n",
            "HalfBath        1460 non-null int64\n",
            "BedroomAbvGr    1460 non-null int64\n",
            "Fireplaces      1460 non-null int64\n",
            "GarageArea      1460 non-null int64\n",
            "PavedDrive      1460 non-null object\n",
            "SalePrice       1460 non-null int64\n",
            "dtypes: float64(1), int64(15), object(5)\n",
            "memory usage: 239.6+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-NXzKTNEweiK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        },
        "outputId": "c63fd7ee-52c0-4cf3-f148-1ef2a821813b"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Fill missing values with 0\n",
        "unproc_house.LotFrontage = unproc_house.LotFrontage.fillna(0)\n",
        "\n",
        "# Create a boolean mask for categorical columns\n",
        "categorical_mask = (unproc_house.dtypes == object)\n",
        "\n",
        "# Get list of categorical column names\n",
        "categorical_columns = unproc_house.columns[categorical_mask].tolist()\n",
        "\n",
        "# Print the head of the categorical columns\n",
        "print(unproc_house[categorical_columns].head())\n",
        "\n",
        "# Create LabelEncoder object: le\n",
        "le = LabelEncoder()\n",
        "\n",
        "# Apply LabelEncoder to categorical columns\n",
        "unproc_house[categorical_columns] = unproc_house[categorical_columns].apply(lambda x: le.fit_transform(x))\n",
        "\n",
        "# Print the head of the LabelEncoded categorical columns\n",
        "print(unproc_house[categorical_columns].head())"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "  MSZoning Neighborhood BldgType HouseStyle PavedDrive\n",
            "0       RL      CollgCr     1Fam     2Story          Y\n",
            "1       RL      Veenker     1Fam     1Story          Y\n",
            "2       RL      CollgCr     1Fam     2Story          Y\n",
            "3       RL      Crawfor     1Fam     2Story          Y\n",
            "4       RL      NoRidge     1Fam     2Story          Y\n",
            "   MSZoning  Neighborhood  BldgType  HouseStyle  PavedDrive\n",
            "0         3             5         0           5           2\n",
            "1         3            24         0           2           2\n",
            "2         3             5         0           5           2\n",
            "3         3             6         0           5           2\n",
            "4         3            15         0           5           2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "EOqKGM2sweiR",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoding categorical columns II: OneHotEncoder\n",
        "Okay - so you have your categorical columns encoded numerically. Can you now move onto using pipelines and XGBoost? Not yet! In the categorical columns of this dataset, there is no natural ordering between the entries. As an example: Using LabelEncoder, the CollgCr Neighborhood was encoded as 5, while the Veenker Neighborhood was encoded as 24, and Crawfor as 6. Is Veenker \"greater\" than Crawfor and CollgCr? No - and allowing the model to assume this natural ordering may result in poor performance.\n",
        "\n",
        "As a result, there is another step needed: You have to apply a one-hot encoding to create binary, or \"dummy\" variables. You can do this using scikit-learn's OneHotEncoder."
      ]
    },
    {
      "metadata": {
        "id": "KjVW4XH4weiR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 816
        },
        "outputId": "8fbf19e8-e65f-4462-a575-d346f0e12d71"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "\n",
        "# Create OneHotEncoder: ohe\n",
        "ohe = OneHotEncoder(categorical_features=categorical_mask,sparse=False)\n",
        "\n",
        "# Apply OneHotEncoder to categorical columns - output is no longer a dataframe: df_encoded\n",
        "df_encoded = ohe.fit_transform(unproc_house)\n",
        "\n",
        "# Print first 5 rows of the resulting dataset - again, this will no longer be a pandas dataframe\n",
        "print(df_encoded[:5, :])\n",
        "\n",
        "# Print the shape of the original DataFrame\n",
        "print(unproc_house.shape)\n",
        "\n",
        "# Print the shape of the transformed array\n",
        "print(df_encoded.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.500e+01 8.450e+03\n",
            "  7.000e+00 5.000e+00 2.003e+03 0.000e+00 1.710e+03 1.000e+00 0.000e+00\n",
            "  2.000e+00 1.000e+00 3.000e+00 0.000e+00 5.480e+02 2.085e+05]\n",
            " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 1.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 2.000e+01 8.000e+01 9.600e+03\n",
            "  6.000e+00 8.000e+00 1.976e+03 0.000e+00 1.262e+03 0.000e+00 1.000e+00\n",
            "  2.000e+00 0.000e+00 3.000e+00 1.000e+00 4.600e+02 1.815e+05]\n",
            " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 6.800e+01 1.125e+04\n",
            "  7.000e+00 5.000e+00 2.001e+03 1.000e+00 1.786e+03 1.000e+00 0.000e+00\n",
            "  2.000e+00 1.000e+00 3.000e+00 1.000e+00 6.080e+02 2.235e+05]\n",
            " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 7.000e+01 6.000e+01 9.550e+03\n",
            "  7.000e+00 5.000e+00 1.915e+03 1.000e+00 1.717e+03 1.000e+00 0.000e+00\n",
            "  1.000e+00 0.000e+00 3.000e+00 1.000e+00 6.420e+02 1.400e+05]\n",
            " [0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 0.000e+00 0.000e+00 1.000e+00 0.000e+00\n",
            "  0.000e+00 0.000e+00 0.000e+00 1.000e+00 6.000e+01 8.400e+01 1.426e+04\n",
            "  8.000e+00 5.000e+00 2.000e+03 0.000e+00 2.198e+03 1.000e+00 0.000e+00\n",
            "  2.000e+00 1.000e+00 4.000e+00 1.000e+00 8.360e+02 2.500e+05]]\n",
            "(1460, 21)\n",
            "(1460, 62)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dTq_sVYcweiU",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Encoding categorical columns III: DictVectorizer\n",
        "Alright, one final trick before you dive into pipelines. The two step process you just went through - LabelEncoder followed by OneHotEncoder - can be simplified by using a DictVectorizer.\n",
        "\n",
        "Using a DictVectorizer on a DataFrame that has been converted to a dictionary allows you to get label encoding as well as one-hot encoding in one go."
      ]
    },
    {
      "metadata": {
        "id": "TPLyvgzhweiX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 309
        },
        "outputId": "deec4814-2e14-4563-cacb-ce96186dfdf7"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction import DictVectorizer\n",
        "\n",
        "# Convert df into a dictionary: df_dict\n",
        "df_dict = unproc_house.to_dict('records')\n",
        "\n",
        "# Create the DictVectorizer object: dv\n",
        "dv = DictVectorizer(sparse=False)\n",
        "\n",
        "# Apply dv on df: df_encoded\n",
        "df_encoded = dv.fit_transform(df_dict)\n",
        "\n",
        "# Print the resulting first five rows\n",
        "print(df_encoded[:5,:])\n",
        "\n",
        "# Print the vocabulary\n",
        "print(dv.vocabulary_)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[3.000e+00 0.000e+00 1.000e+00 0.000e+00 0.000e+00 2.000e+00 5.480e+02\n",
            "  1.710e+03 1.000e+00 5.000e+00 8.450e+03 6.500e+01 6.000e+01 3.000e+00\n",
            "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 0.000e+00 2.085e+05 2.003e+03]\n",
            " [3.000e+00 0.000e+00 0.000e+00 1.000e+00 1.000e+00 2.000e+00 4.600e+02\n",
            "  1.262e+03 0.000e+00 2.000e+00 9.600e+03 8.000e+01 2.000e+01 3.000e+00\n",
            "  2.400e+01 8.000e+00 6.000e+00 2.000e+00 0.000e+00 1.815e+05 1.976e+03]\n",
            " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 6.080e+02\n",
            "  1.786e+03 1.000e+00 5.000e+00 1.125e+04 6.800e+01 6.000e+01 3.000e+00\n",
            "  5.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 2.235e+05 2.001e+03]\n",
            " [3.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 1.000e+00 6.420e+02\n",
            "  1.717e+03 0.000e+00 5.000e+00 9.550e+03 6.000e+01 7.000e+01 3.000e+00\n",
            "  6.000e+00 5.000e+00 7.000e+00 2.000e+00 1.000e+00 1.400e+05 1.915e+03]\n",
            " [4.000e+00 0.000e+00 1.000e+00 0.000e+00 1.000e+00 2.000e+00 8.360e+02\n",
            "  2.198e+03 1.000e+00 5.000e+00 1.426e+04 8.400e+01 6.000e+01 3.000e+00\n",
            "  1.500e+01 5.000e+00 8.000e+00 2.000e+00 0.000e+00 2.500e+05 2.000e+03]]\n",
            "{'MSSubClass': 12, 'MSZoning': 13, 'LotFrontage': 11, 'LotArea': 10, 'Neighborhood': 14, 'BldgType': 1, 'HouseStyle': 9, 'OverallQual': 16, 'OverallCond': 15, 'YearBuilt': 20, 'Remodeled': 18, 'GrLivArea': 7, 'BsmtFullBath': 2, 'BsmtHalfBath': 3, 'FullBath': 5, 'HalfBath': 8, 'BedroomAbvGr': 0, 'Fireplaces': 4, 'GarageArea': 6, 'PavedDrive': 17, 'SalePrice': 19}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "lo8Glu2nweie",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Pipeline"
      ]
    },
    {
      "metadata": {
        "id": "CqnA-evxweig",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_Uhouse=unproc_house.iloc[:,0:-1]\n",
        "y_Uhouse=unproc_house.iloc[:,-1]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "XKsDNb2Sweij",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "6bf510dd-3f3c-4ceb-fe66-111907db9f5c"
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# Fill LotFrontage missing values with 0\n",
        "X_Uhouse.LotFrontage = X_Uhouse.LotFrontage.fillna(0)\n",
        "\n",
        "# Setup the pipeline steps: steps\n",
        "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
        "         (\"xgb_model\", xgb.XGBRegressor())]\n",
        "\n",
        "# Create the pipeline: xgb_pipeline\n",
        "xgb_pipeline = Pipeline(steps)\n",
        "\n",
        "X_dict=X_Uhouse.to_dict('records')\n",
        "# Fit the pipeline\n",
        "xgb_pipeline.fit(X_dict,y_Uhouse)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "     steps=[('ohe_onestep', DictVectorizer(dtype=<class 'numpy.float64'>, separator='=', sort=True,\n",
              "        sparse=False)), ('xgb_model', XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
              "       colsample_bytree=1, gamma=0, learning_rate=0.1, max_delta_step=0,\n",
              "       max_depth=3, min_ch...\n",
              "       reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n",
              "       silent=True, subsample=1))])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "metadata": {
        "id": "ApiRnVxdweil",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Additional Components Introduced For Pipelines\n",
        "* sklearn_pandas:\n",
        "    * DataFrameMapper - Interoperability between pandas and scikit-learn\n",
        "    * CategoricalImputer - Allow for imputation of categorical variables before conversion to integers\n",
        "* sklearn.preprocessing:\n",
        "    * Imputer - Native imputation of numerical columns in scikit-learn\n",
        "* sklearn.pipeline:\n",
        "    * FeatureUnion - combine multiple pipelines of features into a single pipeline of features"
      ]
    },
    {
      "metadata": {
        "id": "dYlUYf0qweim",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f0dd4b91-0c75-4683-884c-5bba481016c5"
      },
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Fill LotFrontage missing values with 0\n",
        "X_Uhouse.LotFrontage = X_Uhouse.LotFrontage.fillna(0)\n",
        "\n",
        "# Setup the pipeline steps: steps\n",
        "steps = [(\"ohe_onestep\", DictVectorizer(sparse=False)),\n",
        "         (\"xgb_model\", xgb.XGBRegressor(max_depth=3, objective=\"reg:linear\"))]\n",
        "\n",
        "# Create the pipeline: xgb_pipeline\n",
        "xgb_pipeline = Pipeline(steps)\n",
        "\n",
        "# Cross-validate the model\n",
        "cross_val_scores = cross_val_score(xgb_pipeline,X_dict,y_Uhouse, scoring='neg_mean_squared_error',cv=10)\n",
        "\n",
        "# Print the 10-fold RMSE\n",
        "print(\"10-fold RMSE: \", np.mean(np.sqrt(np.abs(cross_val_scores))))"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10-fold RMSE:  28440.796697477967\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "_NQZ2h7vweir",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "## Kidney disease case study I: Categorical Imputer\n",
        "You'll now continue your exploration of using pipelines with a dataset that requires significantly more wrangling. The chronic kidney disease dataset contains both categorical and numeric features, but contains lots of missing values. The goal here is to predict who has [chronic kidney disease]('https://archive.ics.uci.edu/ml/datasets/chronic_kidney_disease#') given various blood indicators as features.\n",
        "\n",
        "You'll be introduced to a new library, sklearn_pandas, that allows you to chain many more processing steps inside of a pipeline than are currently supported in scikit-learn. Specifically, you'll be able to impute missing categorical values directly using the `Categorical_Imputer()` class in `sklearn_pandas`, and the `DataFrameMapper()` class to apply any arbitrary sklearn-compatible transformer on DataFrame columns, where the resulting output can be either a NumPy array or DataFrame.\n",
        "\n",
        "In this exercise, your task is to apply the CategoricalImputer to impute all of the categorical columns in the dataset. You can refer to how the numeric imputation mapper was created as a template. Notice the keyword arguments input_df=True and df_out=True? This is so that you can work with DataFrames instead of arrays. By default, the transformers are passed a numpy array of the selected columns as input, and as a result, the output of the DataFrame mapper is also an array. Scikit-learn transformers have historically been designed to work with numpy arrays, not pandas DataFrames, even though their basic indexing interfaces are similar."
      ]
    },
    {
      "metadata": {
        "id": "dQLF06wqweis",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 233
        },
        "outputId": "fd3bf05f-8d28-4a71-ef15-5aed9ae568ab"
      },
      "cell_type": "code",
      "source": [
        "# Some preprocessing\n",
        "features_names=['age','bp','sg','al','su','bgr','bu','sc','sod','pot','hemo','pcv','wc','rc','rbc','pc','pcc','ba', 'htn','dm','cad','appet','pe','ane','class']\n",
        "kidney_disease=pd.read_csv('https://assets.datacamp.com/production/course_3679/datasets/chronic_kidney_disease.csv',header=None,names=features_names,na_values='?')\n",
        "kidney_disease.head()\n"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>age</th>\n",
              "      <th>bp</th>\n",
              "      <th>sg</th>\n",
              "      <th>al</th>\n",
              "      <th>su</th>\n",
              "      <th>bgr</th>\n",
              "      <th>bu</th>\n",
              "      <th>sc</th>\n",
              "      <th>sod</th>\n",
              "      <th>pot</th>\n",
              "      <th>...</th>\n",
              "      <th>pc</th>\n",
              "      <th>pcc</th>\n",
              "      <th>ba</th>\n",
              "      <th>htn</th>\n",
              "      <th>dm</th>\n",
              "      <th>cad</th>\n",
              "      <th>appet</th>\n",
              "      <th>pe</th>\n",
              "      <th>ane</th>\n",
              "      <th>class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>48.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>121.0</td>\n",
              "      <td>...</td>\n",
              "      <td>44.0</td>\n",
              "      <td>7800.0</td>\n",
              "      <td>5.2</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>7.0</td>\n",
              "      <td>50.0</td>\n",
              "      <td>1.020</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>38.0</td>\n",
              "      <td>6000.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>62.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>423.0</td>\n",
              "      <td>...</td>\n",
              "      <td>31.0</td>\n",
              "      <td>7500.0</td>\n",
              "      <td>NaN</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>no</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>48.0</td>\n",
              "      <td>70.0</td>\n",
              "      <td>1.005</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>abnormal</td>\n",
              "      <td>present</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>117.0</td>\n",
              "      <td>...</td>\n",
              "      <td>32.0</td>\n",
              "      <td>6700.0</td>\n",
              "      <td>3.9</td>\n",
              "      <td>yes</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>poor</td>\n",
              "      <td>yes</td>\n",
              "      <td>yes</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>51.0</td>\n",
              "      <td>80.0</td>\n",
              "      <td>1.010</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>normal</td>\n",
              "      <td>normal</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>notpresent</td>\n",
              "      <td>106.0</td>\n",
              "      <td>...</td>\n",
              "      <td>35.0</td>\n",
              "      <td>7300.0</td>\n",
              "      <td>4.6</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>good</td>\n",
              "      <td>no</td>\n",
              "      <td>no</td>\n",
              "      <td>ckd</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows Ã— 25 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "    age    bp     sg   al   su     bgr        bu          sc         sod  \\\n",
              "0  48.0  80.0  1.020  1.0  0.0     NaN    normal  notpresent  notpresent   \n",
              "1   7.0  50.0  1.020  4.0  0.0     NaN    normal  notpresent  notpresent   \n",
              "2  62.0  80.0  1.010  2.0  3.0  normal    normal  notpresent  notpresent   \n",
              "3  48.0  70.0  1.005  4.0  0.0  normal  abnormal     present  notpresent   \n",
              "4  51.0  80.0  1.010  2.0  0.0  normal    normal  notpresent  notpresent   \n",
              "\n",
              "     pot  ...     pc     pcc   ba  htn   dm  cad  appet   pe  ane class  \n",
              "0  121.0  ...   44.0  7800.0  5.2  yes  yes   no   good   no   no   ckd  \n",
              "1    NaN  ...   38.0  6000.0  NaN   no   no   no   good   no   no   ckd  \n",
              "2  423.0  ...   31.0  7500.0  NaN   no  yes   no   poor   no  yes   ckd  \n",
              "3  117.0  ...   32.0  6700.0  3.9  yes   no   no   poor  yes  yes   ckd  \n",
              "4  106.0  ...   35.0  7300.0  4.6   no   no   no   good   no   no   ckd  \n",
              "\n",
              "[5 rows x 25 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "metadata": {
        "id": "YGn9rC1bweiy",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "X_kidney=kidney_disease.iloc[:,0:-1]\n",
        "y_kidney=kidney_disease.iloc[:,-1]\n",
        "\n",
        "# converts to a Dict to be processed in DictVect\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "mRV7XQISwei0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 442
        },
        "outputId": "872aa725-61ed-455c-aeab-6d9d643b0d06"
      },
      "cell_type": "code",
      "source": [
        "# Check number of nulls in each feature column\n",
        "nulls_per_column = X_kidney.isnull().sum()\n",
        "print(nulls_per_column)"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "age        9\n",
            "bp        12\n",
            "sg        47\n",
            "al        46\n",
            "su        49\n",
            "bgr      152\n",
            "bu        65\n",
            "sc         4\n",
            "sod        4\n",
            "pot       44\n",
            "hemo      19\n",
            "pcv       17\n",
            "wc        87\n",
            "rc        88\n",
            "rbc       52\n",
            "pc        71\n",
            "pcc      106\n",
            "ba       131\n",
            "htn        2\n",
            "dm         2\n",
            "cad        2\n",
            "appet      1\n",
            "pe         1\n",
            "ane        1\n",
            "dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "souOx_65_nmM",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "outputId": "00c441f0-8600-478e-a61f-1f7aac4c1d95"
      },
      "cell_type": "code",
      "source": [
        "!pip install sklearn-pandas"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting sklearn-pandas\n",
            "  Downloading https://files.pythonhosted.org/packages/7e/9c/c94f46b40b86d2c77c46c4c1b858fc66c117b4390665eca28f2e0812db45/sklearn_pandas-1.7.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: pandas>=0.11.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (0.22.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (0.19.1)\n",
            "Requirement already satisfied: scikit-learn>=0.15.0 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (0.19.2)\n",
            "Requirement already satisfied: numpy>=1.6.1 in /usr/local/lib/python3.6/dist-packages (from sklearn-pandas) (1.14.6)\n",
            "Requirement already satisfied: pytz>=2011k in /usr/local/lib/python3.6/dist-packages (from pandas>=0.11.0->sklearn-pandas) (2018.6)\n",
            "Requirement already satisfied: python-dateutil>=2 in /usr/local/lib/python3.6/dist-packages (from pandas>=0.11.0->sklearn-pandas) (2.5.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2->pandas>=0.11.0->sklearn-pandas) (1.11.0)\n",
            "Installing collected packages: sklearn-pandas\n",
            "Successfully installed sklearn-pandas-1.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Ph6VukUwei3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn_pandas import DataFrameMapper\n",
        "from sklearn_pandas import CategoricalImputer\n",
        "from sklearn.preprocessing import Imputer\n",
        "\n",
        "# Create a boolean mask for categorical columns\n",
        "categorical_feature_mask = X_kidney.dtypes == object\n",
        "\n",
        "# Get list of categorical column names\n",
        "categorical_columns = X_kidney.columns[categorical_feature_mask].tolist()\n",
        "\n",
        "# Get list of non-categorical column names\n",
        "non_categorical_columns = X_kidney.columns[~categorical_feature_mask].tolist()\n",
        "\n",
        "# Apply numeric imputer\n",
        "numeric_imputation_mapper = DataFrameMapper(\n",
        "                                            [([numeric_feature], Imputer(strategy=\"median\")) for numeric_feature in non_categorical_columns],\n",
        "                                            input_df=True,\n",
        "                                            df_out=True\n",
        "                                           )\n",
        "\n",
        "# Apply categorical imputer\n",
        "categorical_imputation_mapper = DataFrameMapper(\n",
        "                                                [(category_feature, CategoricalImputer()) for category_feature in categorical_columns],\n",
        "                                                input_df=True,\n",
        "                                                df_out=True\n",
        "                                               )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "3djIKNDhwei7",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Kidney disease case study II: Feature Union\n",
        "Having separately imputed numeric as well as categorical columns, your task is now to use scikit-learn's FeatureUnion to concatenate their results, which are contained in two separate transformer objects - numeric_imputation_mapper, and categorical_imputation_mapper, respectively."
      ]
    },
    {
      "metadata": {
        "id": "6ySniKmVwei9",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import FeatureUnion\n",
        "\n",
        "# Combine the numeric and categorical transformations\n",
        "numeric_categorical_union = FeatureUnion([\n",
        "                                          (\"num_mapper\", numeric_imputation_mapper),\n",
        "                                          (\"cat_mapper\", categorical_imputation_mapper)\n",
        "                                         ])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "4-h7gUvpwei-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "### Kidney disease case study III: Full pipeline\n",
        "It's time to piece together all of the transforms along with an XGBClassifier to build the full pipeline!\n",
        "\n",
        "Besides the numeric_categorical_union that you created in the previous exercise, there are two other transforms needed: the Dictifier() transform which we created for you, and the DictVectorizer()."
      ]
    },
    {
      "metadata": {
        "id": "Sk8AKqbBwei_",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.preprocessing import LabelBinarizer\n",
        "from sklearn.preprocessing import Imputer, LabelBinarizer\n",
        "\n",
        "#helper class to allow DictVectorizer to work properly in pipeline with dataframe\n",
        "class Dictifier(BaseEstimator,TransformerMixin):\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "    def transform(self, X):\n",
        "        if type(X) == pd.core.frame.DataFrame:\n",
        "            return X.to_dict(\"records\")\n",
        "        else:\n",
        "            return pd.DataFrame(X).to_dict(\"records\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "gqUqABEiwejC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "kidney_columns = [\"age\",\"bp\",\"sg\",\"al\",\"su\",\"rbc\",\"pc\",\"pcc\",\"ba\",\"bgr\",\"bu\",\"sc\",\"sod\",\"pot\",\"hemo\",\"pcv\",\"wc\",\"rc\",\"htn\",\"dm\",\"cad\",\"appet\",\"pe\",\"ane\",\"class\"]\n",
        "kidney_data = pd.read_csv(\"https://s3.amazonaws.com/assets.datacamp.com/production/course_3970/datasets/chronic_kidney_disease.csv\", header=None,names=kidney_columns,na_values=\"?\")\n",
        "\n",
        "kidney_columns = kidney_columns[:5]+kidney_columns[9:18]+kidney_columns[5:9]+kidney_columns[18:]\n",
        "kidney_data = kidney_data[kidney_columns]\n",
        "\n",
        "kidney_feature_names = kidney_data.columns.tolist()[:-1]\n",
        "kidney_target_name = kidney_data.columns.tolist()[-1]\n",
        "\n",
        "X,y = kidney_data[kidney_feature_names],kidney_data[kidney_target_name]\n",
        "\n",
        "#convert target to numeric and reshape as cross_val_score only takes dimensionless arrays, not column vectors as input\n",
        "y = LabelBinarizer().fit_transform(y)\n",
        "y = y.reshape(y.shape[0],)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lJ-JfCLCwejJ",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Create full pipeline\n",
        "pipeline = Pipeline([\n",
        "                     (\"featureunion\", numeric_categorical_union),\n",
        "                     (\"dictifier\", Dictifier()),\n",
        "                     (\"vectorizer\", DictVectorizer(sort=False)),\n",
        "                     (\"clf\", xgb.XGBClassifier(max_depth=3))\n",
        "                    ])\n",
        "\n",
        "\n",
        "\n",
        "#cross_val_scores = cross_val_score(pipeline, X, y, scoring=\"roc_auc\", cv=3)\n",
        "\n",
        "# Print avg. AUC\n",
        "#print(\"3-fold AUC: \", np.mean(cross_val_scores))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "DO85qF2owejO",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "gbm_param_grid = {\n",
        "    'clf__learning_rate': np.arange(0.05, 1, 0.05),\n",
        "    'clf__max_depth': np.arange(3, 10, 1),\n",
        "    'clf__n_estimators': np.arange(50, 200,50)\n",
        "}\n",
        "\n",
        "# Perform RandomizedSearchCV\n",
        "randomized_roc_auc = RandomizedSearchCV(estimator=pipeline,param_distributions=gbm_param_grid,n_iter=2,scoring='roc_auc',cv=2,verbose=1)\n",
        "\n",
        "# Fit the estimator\n",
        "randomized_roc_auc.fit(X,y)\n",
        "\n",
        "# Compute metrics\n",
        "print(randomized_roc_auc.best_score_)\n",
        "print(randomized_roc_auc.best_estimator_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Epg-IiRswejR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}